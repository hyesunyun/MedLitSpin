{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Testing Prompts with GPT-4o\n",
    "\n",
    "This is for generating synthetic dataset which involves having the LLM (GPT-4o in this case) add different magnitude of spin to a given abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "from json import dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_keys():\n",
    "    load_dotenv(override=True)\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts in DEV set: 14\n"
     ]
    }
   ],
   "source": [
    "# get original abstracts used for testing prompts (DEV set)\n",
    "df_abstracts = pd.read_csv('../data/FactPico_115_abstracts_with_pmid_pmcid_title.csv')\n",
    "df_dev_abstracts = df_abstracts[df_abstracts['data_split'] == 'DEV']\n",
    "\n",
    "# FOR TESTING (TODO: remove)\n",
    "# df_dev_abstracts = df_dev_abstracts.sample(3)\n",
    "\n",
    "# count number of abstracts\n",
    "print('Number of abstracts in DEV set:', len(df_dev_abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART TO CUSTOMIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLULAR_EXAMPLE_FORMAT_TYPE = 'markdown' # 'json' or 'tsv' or 'markdown'\n",
    "\n",
    "examples_df = pd.read_csv('../data/example_w_wo_spin_boutron_2014.csv')  # load the examples\n",
    "\n",
    "if TABLULAR_EXAMPLE_FORMAT_TYPE == 'json':\n",
    "    EXAMPLES = dumps(examples_df.to_dict(orient='records'))\n",
    "elif TABLULAR_EXAMPLE_FORMAT_TYPE == 'tsv':\n",
    "    EXAMPLES = examples_df.to_csv(index=False, sep='\\t')\n",
    "elif TABLULAR_EXAMPLE_FORMAT_TYPE == 'markdown':\n",
    "    EXAMPLES = examples_df.to_markdown(index=False)\n",
    "else:\n",
    "    EXAMPLES = dumps(examples_df.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_NAME = 'gpt4o-guideline-1-example-remove-spin-markdown-temp0_6-p0_7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = 'You are an expert in the field of health sciences.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = '''Rewrite the provided abstract to remove any instances of spin. The length of the new abstract should be within 25 words of the original. Do not make any changes if there are no instances of spin in the original. Follow the guidelines below for removing spin from an abstract.\n",
    "Guidelines for writing an abstract without spin:\n",
    "* In the Context section:\n",
    "    * Delete all information that could distort the understanding of the aim of the trial.\n",
    "        * The aim is to evaluate the treatment effect on a secondary outcome.\n",
    "        * The aim is to evaluate the treatment effect for a subgroup.\n",
    "        * The aim is to evaluate overall improvement.\n",
    "* In the Methods section:\n",
    "    * Clearly report the primary outcome.\n",
    "    * According to space constraints, report all secondary outcomes evaluated in the Methods section or report no secondary outcome evaluated in the Methods section to avoid specifically highlighting statistically significant secondary outcomes.\n",
    "    * Delete information that could distort the understanding of the aim (eg, within-group comparison, modified population analysis, subgroup analysis).\n",
    "* In the Results section:\n",
    "    * Delete subgroup analyses that were not prespecified, based on the primary outcome, and interpreted in light of the totality of prespecified subgroup analyses undertaken.\n",
    "    * Delete within-group comparisons.\n",
    "    * Delete linguistic spin.\n",
    "    * Report the results for the primary outcome with numbers in both arms (if possible with some measure of variability) with no wording of judgment.\n",
    "    * Report results for all secondary outcomes, for no secondary outcome, or for the most clinically important secondary outcome.\n",
    "    * Report safety data including reason for withdrawals; report treatment discontinuation when applicable.\n",
    "* In the Conclusions section:\n",
    "    * Delete the author conclusion, and only add the following standardized conclusion: “the treatment A was not more effective than comparator B in patients with….”\n",
    "    * Specify the primary outcome in the conclusion when some secondary outcomes were statistically significant: “the treatment A was not more effective on overall survival than the comparator B in patients with….”\n",
    "\n",
    "Example: {examples}\n",
    "\n",
    "Abstract: {abstract}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.6\n",
    "TOP_P = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_FILENAME = \"./prompt_engineering/\" + PROMPT_TEMPLATE_NAME + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Severity(Enum):\n",
    "    subtle = 'subtle'\n",
    "    moderate = 'moderate'\n",
    "    strong = 'strong'\n",
    "\n",
    "# Used for generating rationales for each edit\n",
    "class Documentation(BaseModel):\n",
    "    edit: str\n",
    "    rationale: str\n",
    "    strategy_applied: str\n",
    "\n",
    "class AbstractResponse(BaseModel):\n",
    "    title: str\n",
    "    abstract: str\n",
    "    spin_severity: Severity\n",
    "    documentation: list[Documentation]\n",
    "\n",
    "class Response(BaseModel):\n",
    "    generated: list[AbstractResponse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_output(model_output_response: Response):\n",
    "    formatted_output = \"\"\n",
    "    for response in model_output_response.generated:\n",
    "        formatted_documentation = \"\"\n",
    "        for documentation in response.documentation:\n",
    "            formatted_documentation += f\"Edit: {documentation.edit}\\nRationale: {documentation.rationale}\\nStrategy Applied: {documentation.strategy_applied},\\n\"\n",
    "        formatted_output += f\"{response.spin_severity.value.capitalize()}:\\nTitle: {response.title}\\nAbstract: {response.abstract}\\nDocumentation: [{formatted_documentation}]\\n\\n\"\n",
    "\n",
    "    return formatted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gpt4o(title, abstract, client, temperature=1, top_p=1):\n",
    "    \n",
    "    sys_prompt = SYSTEM_PROMPT.replace('{title}', title).replace('{abstract}', abstract).replace('{examples}', EXAMPLES)\n",
    "    user_prompt = PROMPT_TEMPLATE.replace('{title}', title).replace('{abstract}', abstract).replace('{examples}', EXAMPLES)\n",
    "    try:\n",
    "        # TODO\n",
    "        # response = client.beta.chat.completions.parse(\n",
    "        #     model=\"gpt-4o\",\n",
    "        #     temperature=temperature,\n",
    "        #     top_p=top_p,\n",
    "        #     messages=[\n",
    "        #         {'role':'system', 'content': sys_prompt},\n",
    "        #         {'role': 'user', 'content': user_prompt}\n",
    "        #     ],\n",
    "        #     response_format=Response,\n",
    "        # )\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            messages=[\n",
    "                {'role':'system', 'content': sys_prompt},\n",
    "                {'role': 'user', 'content': user_prompt}\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    # TODO\n",
    "    #     response_message = response.choices[0].message\n",
    "    #     if response_message.parsed:\n",
    "    #         response = response_message.parsed\n",
    "    #         return response \n",
    "    #     elif response_message.refusal:\n",
    "    #         # handle refusal\n",
    "    #         print(response_message.refusal)\n",
    "    except Exception as e:\n",
    "        # Handle exceptions\n",
    "        print(e)\n",
    "        pass   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:35<00:00,  6.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# prompt_template, prompt_with_input, model, model_output\n",
    "client = load_api_keys()\n",
    "\n",
    "output_data = []\n",
    "for i, row in tqdm(df_dev_abstracts.iterrows(), total=df_dev_abstracts.shape[0]):\n",
    "    data_dict = {}\n",
    "    data_dict['pmid'] = row['pmid']\n",
    "    data_dict['pmcid'] = row['pmcid']\n",
    "    data_dict['title'] = row['title']\n",
    "    data_dict['abstract'] = row['abstract']\n",
    "    data_dict['prompt_template_name'] = PROMPT_TEMPLATE_NAME\n",
    "    data_dict['prompt_template'] = 'system prompt: ' + SYSTEM_PROMPT + ' user prompt: ' + PROMPT_TEMPLATE\n",
    "    data_dict['model_name'] = 'gpt-4o'\n",
    "    data_dict['temperature'] = TEMPERATURE\n",
    "    data_dict['top_p'] = TOP_P\n",
    "    model_output = gen_gpt4o(row['title'], row['abstract'], client, TEMPERATURE, TOP_P)\n",
    "    # data_dict['model_output'] = format_model_output(model_output)\n",
    "    # TODO\n",
    "    data_dict['model_output'] = model_output\n",
    "\n",
    "    output_data.append(data_dict)\n",
    "    \n",
    "new_df = pd.DataFrame.from_dict(output_data)\n",
    "\n",
    "new_df.to_csv(NEW_FILENAME, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedLitSpin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
