
     active environment : MedLitSpin
    active env location : /home/yun.hy/.conda/envs/MedLitSpin
            shell level : 2
       user config file : /home/yun.hy/.condarc
 populated config files : 
          conda version : 4.5.4
    conda-build version : 3.10.5
         python version : 3.6.5.final.0
       base environment : /shared/centos7/anaconda3/3.6  (read only)
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/free/linux-64
                          https://repo.anaconda.com/pkgs/free/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
                          https://repo.anaconda.com/pkgs/pro/linux-64
                          https://repo.anaconda.com/pkgs/pro/noarch
          package cache : /shared/centos7/anaconda3/3.6/pkgs
                          /home/yun.hy/.conda/pkgs
       envs directories : /home/yun.hy/.conda/envs
                          /shared/centos7/anaconda3/3.6/envs
               platform : linux-64
             user-agent : conda/4.5.4 requests/2.18.4 CPython/3.6.5 Linux/3.10.0-1160.25.1.el7.x86_64 centos/7 glibc/2.17
                UID:GID : 1825635949:100
             netrc file : /home/yun.hy/.netrc
           offline mode : False

Running evaluation for interpreting trial results from vanilla PLS using the evaluator models...
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/gpt35/gpt35_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gpt35
Is Debug:     None

Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gpt35/gpt35_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gpt35/gpt35_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6333333333333333
Mean difference for 'rigor_answer': 1.333333333333333
Mean difference for 'importance_answer': 2.8000000000000003
Mean difference for 'full_text_answer': 3.766666666666667
Mean difference for 'another_trial_answer': 3.9000000000000004

Overall mean difference across all answers: 3.086666666666667
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/gpt4o/gpt4o_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gpt4o
Is Debug:     None

Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gpt4o/gpt4o_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gpt4o/gpt4o_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6
Mean difference for 'rigor_answer': 1.3999999999999995
Mean difference for 'importance_answer': 2.7666666666666666
Mean difference for 'full_text_answer': 3.6
Mean difference for 'another_trial_answer': 3.833333333333333

Overall mean difference across all answers: 3.04
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/gpt4o-mini/gpt4o-mini_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gpt4o-mini
Is Debug:     None

Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gpt4o-mini/gpt4o-mini_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gpt4o-mini/gpt4o-mini_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.5333333333333337
Mean difference for 'rigor_answer': 1.3666666666666663
Mean difference for 'importance_answer': 2.8
Mean difference for 'full_text_answer': 3.8000000000000003
Mean difference for 'another_trial_answer': 3.9000000000000004

Overall mean difference across all answers: 3.08
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/gemini_1.5_flash/gemini_1.5_flash_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gemini_1.5_flash
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gemini_1.5_flash/gemini_1_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gemini_1.5_flash/gemini_1_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6333333333333333
Mean difference for 'rigor_answer': 1.333333333333333
Mean difference for 'importance_answer': 2.8000000000000003
Mean difference for 'full_text_answer': 3.7666666666666666
Mean difference for 'another_trial_answer': 3.8

Overall mean difference across all answers: 3.0666666666666664
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/gemini_1.5_flash-8B/gemini_1.5_flash-8B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gemini_1.5_flash-8B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gemini_1.5_flash-8B/gemini_1_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/gemini_1.5_flash-8B/gemini_1_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6
Mean difference for 'rigor_answer': 1.3666666666666663
Mean difference for 'importance_answer': 2.666666666666667
Mean difference for 'full_text_answer': 3.6333333333333333
Mean difference for 'another_trial_answer': 3.833333333333333

Overall mean difference across all answers: 3.02
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/claude_3.5-sonnet/claude_3.5-sonnet_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/claude_3.5-sonnet
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/claude_3.5-sonnet/claude_3_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/claude_3.5-sonnet/claude_3_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.5333333333333337
Mean difference for 'rigor_answer': 1.2999999999999998
Mean difference for 'importance_answer': 2.833333333333333
Mean difference for 'full_text_answer': 3.6666666666666665
Mean difference for 'another_trial_answer': 4.0

Overall mean difference across all answers: 3.0666666666666664
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/claude_3.5-haiku/claude_3.5-haiku_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/claude_3.5-haiku
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/claude_3.5-haiku/claude_3_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/claude_3.5-haiku/claude_3_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.5333333333333337
Mean difference for 'rigor_answer': 1.333333333333333
Mean difference for 'importance_answer': 2.7666666666666666
Mean difference for 'full_text_answer': 3.666666666666667
Mean difference for 'another_trial_answer': 3.8

Overall mean difference across all answers: 3.02
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/olmo2_instruct-7B/olmo2_instruct-7B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/olmo2_instruct-7B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/olmo2_instruct-7B/olmo2_instruct-7B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/olmo2_instruct-7B/olmo2_instruct-7B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.5000000000000004
Mean difference for 'rigor_answer': 1.4000000000000004
Mean difference for 'importance_answer': 2.7333333333333334
Mean difference for 'full_text_answer': 3.766666666666667
Mean difference for 'another_trial_answer': 3.9

Overall mean difference across all answers: 3.06
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/olmo2_instruct-13B/olmo2_instruct-13B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/olmo2_instruct-13B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/olmo2_instruct-13B/olmo2_instruct-13B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/olmo2_instruct-13B/olmo2_instruct-13B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.566666666666667
Mean difference for 'rigor_answer': 1.4000000000000004
Mean difference for 'importance_answer': 2.7666666666666666
Mean difference for 'full_text_answer': 3.6333333333333333
Mean difference for 'another_trial_answer': 3.766666666666666

Overall mean difference across all answers: 3.026666666666667
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/mistral_instruct7B/mistral_instruct7B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/mistral_instruct7B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/mistral_instruct7B/mistral_instruct7B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/mistral_instruct7B/mistral_instruct7B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6
Mean difference for 'rigor_answer': 1.3666666666666663
Mean difference for 'importance_answer': 2.7333333333333334
Mean difference for 'full_text_answer': 3.7000000000000006
Mean difference for 'another_trial_answer': 3.866666666666667

Overall mean difference across all answers: 3.0533333333333337
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/med42-8B/med42-8B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/med42-8B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/med42-8B/med42-8B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/med42-8B/med42-8B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6666666666666665
Mean difference for 'rigor_answer': 1.2666666666666666
Mean difference for 'importance_answer': 2.7
Mean difference for 'full_text_answer': 3.6666666666666665
Mean difference for 'another_trial_answer': 3.866666666666667

Overall mean difference across all answers: 3.033333333333333
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/biomistral7B/biomistral7B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/biomistral7B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/biomistral7B/biomistral7B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/biomistral7B/biomistral7B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.566666666666667
Mean difference for 'rigor_answer': 1.2666666666666666
Mean difference for 'importance_answer': 2.7333333333333334
Mean difference for 'full_text_answer': 3.733333333333333
Mean difference for 'another_trial_answer': 3.8

Overall mean difference across all answers: 3.02
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/openbiollm-8B/openbiollm-8B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/openbiollm-8B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/openbiollm-8B/openbiollm-8B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/openbiollm-8B/openbiollm-8B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6
Mean difference for 'rigor_answer': 1.2999999999999998
Mean difference for 'importance_answer': 2.7666666666666666
Mean difference for 'full_text_answer': 3.566666666666667
Mean difference for 'another_trial_answer': 3.9000000000000004

Overall mean difference across all answers: 3.026666666666667
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/llama2_chat-7B/llama2_chat-7B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama2_chat-7B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama2_chat-7B/llama2_chat-7B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama2_chat-7B/llama2_chat-7B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6333333333333333
Mean difference for 'rigor_answer': 1.333333333333333
Mean difference for 'importance_answer': 2.666666666666667
Mean difference for 'full_text_answer': 3.533333333333333
Mean difference for 'another_trial_answer': 3.9333333333333336

Overall mean difference across all answers: 3.02
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/llama2_chat-13B/llama2_chat-13B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama2_chat-13B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama2_chat-13B/llama2_chat-13B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama2_chat-13B/llama2_chat-13B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6333333333333333
Mean difference for 'rigor_answer': 1.3999999999999995
Mean difference for 'importance_answer': 2.7
Mean difference for 'full_text_answer': 3.6333333333333337
Mean difference for 'another_trial_answer': 3.8

Overall mean difference across all answers: 3.033333333333333
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/llama3_instruct-8B/llama3_instruct-8B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama3_instruct-8B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama3_instruct-8B/llama3_instruct-8B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama3_instruct-8B/llama3_instruct-8B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.566666666666667
Mean difference for 'rigor_answer': 1.3999999999999995
Mean difference for 'importance_answer': 2.7
Mean difference for 'full_text_answer': 3.6333333333333333
Mean difference for 'another_trial_answer': 3.966666666666667

Overall mean difference across all answers: 3.0533333333333332
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/llama2_chat-70B/llama2_chat-70B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama2_chat-70B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama2_chat-70B/llama2_chat-70B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama2_chat-70B/llama2_chat-70B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6
Mean difference for 'rigor_answer': 1.4666666666666668
Mean difference for 'importance_answer': 2.6333333333333337
Mean difference for 'full_text_answer': 3.733333333333334
Mean difference for 'another_trial_answer': 3.9333333333333336

Overall mean difference across all answers: 3.0733333333333337
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/llama3_instruct-70B/llama3_instruct-70B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama3_instruct-70B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama3_instruct-70B/llama3_instruct-70B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/llama3_instruct-70B/llama3_instruct-70B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.5333333333333337
Mean difference for 'rigor_answer': 1.2333333333333334
Mean difference for 'importance_answer': 2.7333333333333334
Mean difference for 'full_text_answer': 3.6666666666666665
Mean difference for 'another_trial_answer': 3.966666666666667

Overall mean difference across all answers: 3.026666666666667
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/med42-70B/med42-70B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/med42-70B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/med42-70B/med42-70B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/med42-70B/med42-70B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6666666666666665
Mean difference for 'rigor_answer': 1.333333333333333
Mean difference for 'importance_answer': 2.666666666666667
Mean difference for 'full_text_answer': 3.766666666666666
Mean difference for 'another_trial_answer': 3.8666666666666663

Overall mean difference across all answers: 3.0599999999999996
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/openbiollm-70B/openbiollm-70B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/openbiollm-70B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/openbiollm-70B/openbiollm-70B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/openbiollm-70B/openbiollm-70B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6
Mean difference for 'rigor_answer': 1.2999999999999998
Mean difference for 'importance_answer': 2.666666666666667
Mean difference for 'full_text_answer': 3.6999999999999997
Mean difference for 'another_trial_answer': 3.866666666666667

Overall mean difference across all answers: 3.026666666666667
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/biomedgpt7B/biomedgpt7B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/biomedgpt7B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/biomedgpt7B/biomedgpt7B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/biomedgpt7B/biomedgpt7B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.566666666666667
Mean difference for 'rigor_answer': 1.3666666666666671
Mean difference for 'importance_answer': 2.7
Mean difference for 'full_text_answer': 3.6999999999999997
Mean difference for 'another_trial_answer': 3.833333333333333

Overall mean difference across all answers: 3.033333333333333
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Input Path:   code/pls_outputs/alpacare-7B/alpacare-7B_outputs.csv
Output Path:  code/pls_outputs/_interpretation_eval_results/gpt4o-mini/alpacare-7B
Is Debug:     None

Output path did not exist. Directory was created.
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/_interpretation_eval_results/gpt4o-mini/alpacare-7B/alpacare-7B_outputs_pls_interpretation_outputs.json and code/pls_outputs/_interpretation_eval_results/gpt4o-mini/alpacare-7B/alpacare-7B_outputs_pls_interpretation_outputs.csv
Calculating means differences in scores between the PLS from spun and unspun abstracts...
Mean difference for 'benefit_answer': 3.6333333333333333
Mean difference for 'rigor_answer': 1.333333333333333
Mean difference for 'importance_answer': 2.7666666666666666
Mean difference for 'full_text_answer': 3.8000000000000003
Mean difference for 'another_trial_answer': 3.7666666666666666

Overall mean difference across all answers: 3.06
