
     active environment : MedLitSpin
    active env location : /home/yun.hy/.conda/envs/MedLitSpin
            shell level : 2
       user config file : /home/yun.hy/.condarc
 populated config files : 
          conda version : 24.5.0
    conda-build version : 24.5.1
         python version : 3.12.4.final.0
                 solver : libmamba (default)
       virtual packages : __archspec=1=sapphirerapids
                          __conda=24.5.0=0
                          __cuda=12.3=0
                          __glibc=2.34=0
                          __linux=5.14.0=0
                          __unix=0=0
       base environment : /shared/EL9/explorer/anaconda3/2024.06  (read only)
      conda av data dir : /shared/EL9/explorer/anaconda3/2024.06/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /shared/EL9/explorer/anaconda3/2024.06/pkgs
                          /home/yun.hy/.conda/pkgs
       envs directories : /home/yun.hy/.conda/envs
                          /shared/EL9/explorer/anaconda3/2024.06/envs
               platform : linux-64
             user-agent : conda/24.5.0 requests/2.32.2 CPython/3.12.4 Linux/5.14.0-362.13.1.el9_3.x86_64 rocky/9.3 glibc/2.34 solver/libmamba conda-libmamba-solver/24.1.0 libmambapy/1.5.8 aau/0.4.4 c/. s/. e/.
                UID:GID : 1825635949:100
             netrc file : /home/yun.hy/.netrc
           offline mode : False


Running evaluation for detecting spin in abstracts of medical literature...
Arguments Provided for the Evaluator:
Model:        olmo2_instruct-7B
Output Path:  code/eval_outputs/olmo2_instruct-7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}

Saving outputs from model - olmo2_instruct-7B to csv and json
Model outputs saved to code/eval_outputs/olmo2_instruct-7B/olmo2_instruct-7B_detection_outputs.json and code/eval_outputs/olmo2_instruct-7B/olmo2_instruct-7B_detection_outputs.csv
Calculating metrics...
Accuracy: 0.7
Precision: 1.0
Recall: 0.4
F1: 0.5714285714285714
Arguments Provided for the Evaluator:
Model:        olmo2_instruct-13B
Output Path:  code/eval_outputs/olmo2_instruct-13B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.norm': 1, 'lm_head': 1}

Saving outputs from model - olmo2_instruct-13B to csv and json
Model outputs saved to code/eval_outputs/olmo2_instruct-13B/olmo2_instruct-13B_detection_outputs.json and code/eval_outputs/olmo2_instruct-13B/olmo2_instruct-13B_detection_outputs.csv
Calculating metrics...
Accuracy: 0.5166666666666667
Precision: 1.0
Recall: 0.03333333333333333
F1: 0.06451612903225806
Arguments Provided for the Evaluator:
Model:        mistral_instruct7B
Output Path:  code/eval_outputs/mistral_instruct7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}

Saving outputs from model - mistral_instruct7B to csv and json
Model outputs saved to code/eval_outputs/mistral_instruct7B/mistral_instruct7B_detection_outputs.json and code/eval_outputs/mistral_instruct7B/mistral_instruct7B_detection_outputs.csv
Calculating metrics...
Accuracy: 0.5
Precision: 0.0
Recall: 0.0
F1: 0.0
Arguments Provided for the Evaluator:
Model:        med42-8B
Output Path:  code/eval_outputs/med42-8B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}

Saving outputs from model - med42-8B to csv and json
Model outputs saved to code/eval_outputs/med42-8B/med42-8B_detection_outputs.json and code/eval_outputs/med42-8B/med42-8B_detection_outputs.csv
Calculating metrics...
Accuracy: 0.5833333333333334
Precision: 1.0
Recall: 0.16666666666666666
F1: 0.2857142857142857
Arguments Provided for the Evaluator:
Model:        biomistral7B
Output Path:  code/eval_outputs/biomistral7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}

Saving outputs from model - biomistral7B to csv and json
Model outputs saved to code/eval_outputs/biomistral7B/biomistral7B_detection_outputs.json and code/eval_outputs/biomistral7B/biomistral7B_detection_outputs.csv
Calculating metrics...
Accuracy: 0.5166666666666667
Precision: 1.0
Recall: 0.03333333333333333
F1: 0.06451612903225806
Arguments Provided for the Evaluator:
Model:        openbiollm-8B
Output Path:  code/eval_outputs/openbiollm-8B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}

Saving outputs from model - openbiollm-8B to csv and json
Model outputs saved to code/eval_outputs/openbiollm-8B/openbiollm-8B_detection_outputs.json and code/eval_outputs/openbiollm-8B/openbiollm-8B_detection_outputs.csv
Calculating metrics...
Model's output has some 'Error' or empty string values. Removing these rows from the metrics...
Number of rows after removing 'Error' or empty string values: 59
Accuracy: 0.5084745762711864
Precision: 0.5
Recall: 1.0
F1: 0.6666666666666666
Arguments Provided for the Evaluator:
Model:        llama2_chat-13B
Output Path:  code/eval_outputs/llama2_chat-13B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}

Saving outputs from model - llama2_chat-13B to csv and json
Model outputs saved to code/eval_outputs/llama2_chat-13B/llama2_chat-13B_detection_outputs.json and code/eval_outputs/llama2_chat-13B/llama2_chat-13B_detection_outputs.csv
Calculating metrics...
Accuracy: 0.5666666666666667
Precision: 0.5370370370370371
Recall: 0.9666666666666667
F1: 0.6904761904761905
Arguments Provided for the Evaluator:
Model:        llama3_instruct-8B
Output Path:  code/eval_outputs/llama3_instruct-8B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}

Saving outputs from model - llama3_instruct-8B to csv and json
Model outputs saved to code/eval_outputs/llama3_instruct-8B/llama3_instruct-8B_detection_outputs.json and code/eval_outputs/llama3_instruct-8B/llama3_instruct-8B_detection_outputs.csv
Calculating metrics...
Accuracy: 0.8333333333333334
Precision: 1.0
Recall: 0.6666666666666666
F1: 0.8
Running evaluation for interpreting trial results...
Arguments Provided for the Evaluator:
Model:        olmo2_instruct-7B
Output Path:  code/eval_outputs/olmo2_instruct-7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}

Saving outputs from model - olmo2_instruct-7B to csv and json
Model outputs saved to code/eval_outputs/olmo2_instruct-7B/olmo2_instruct-7B_interpretation_outputs.json and code/eval_outputs/olmo2_instruct-7B/olmo2_instruct-7B_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['16314619', '21060024', '17134892']
Number of rows after removing 'Error' or empty string values: 54
Mean difference for 'benefit_answer': 3.2592592592592586
Mean difference for 'rigor_answer': 0.4666666666666668
Mean difference for 'importance_answer': 1.3666666666666671
Mean difference for 'full_text_answer': 2.3
Mean difference for 'another_trial_answer': 2.7

Overall mean difference across all answers: 2.0185185185185186
Arguments Provided for the Evaluator:
Model:        olmo2_instruct-13B
Output Path:  code/eval_outputs/olmo2_instruct-13B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.norm': 1, 'lm_head': 1}

Saving outputs from model - olmo2_instruct-13B to csv and json
Model outputs saved to code/eval_outputs/olmo2_instruct-13B/olmo2_instruct-13B_interpretation_outputs.json and code/eval_outputs/olmo2_instruct-13B/olmo2_instruct-13B_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 6.0
Mean difference for 'rigor_answer': 0.40000000000000036
Mean difference for 'importance_answer': 2.2333333333333325
Mean difference for 'full_text_answer': 5.466666666666667
Mean difference for 'another_trial_answer': 3.333333333333333

Overall mean difference across all answers: 3.4866666666666664
Arguments Provided for the Evaluator:
Model:        mistral_instruct7B
Output Path:  code/eval_outputs/mistral_instruct7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}

Saving outputs from model - mistral_instruct7B to csv and json
Model outputs saved to code/eval_outputs/mistral_instruct7B/mistral_instruct7B_interpretation_outputs.json and code/eval_outputs/mistral_instruct7B/mistral_instruct7B_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 3.9000000000000004
Mean difference for 'rigor_answer': 0.0
Mean difference for 'importance_answer': 2.466666666666667
Mean difference for 'full_text_answer': 2.466666666666667
Mean difference for 'another_trial_answer': 2.633333333333333

Overall mean difference across all answers: 2.2933333333333334
Arguments Provided for the Evaluator:
Model:        med42-8B
Output Path:  code/eval_outputs/med42-8B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}

Saving outputs from model - med42-8B to csv and json
Model outputs saved to code/eval_outputs/med42-8B/med42-8B_interpretation_outputs.json and code/eval_outputs/med42-8B/med42-8B_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.966666666666667
Mean difference for 'rigor_answer': 0.33333333333333304
Mean difference for 'importance_answer': 1.5999999999999996
Mean difference for 'full_text_answer': 1.6333333333333329
Mean difference for 'another_trial_answer': 3.033333333333333

Overall mean difference across all answers: 1.913333333333333
Arguments Provided for the Evaluator:
Model:        biomistral7B
Output Path:  code/eval_outputs/biomistral7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'lm_head': 1}

Saving outputs from model - biomistral7B to csv and json
Model outputs saved to code/eval_outputs/biomistral7B/biomistral7B_interpretation_outputs.json and code/eval_outputs/biomistral7B/biomistral7B_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['21060024', '20530276', '16148021', '17134892', '22112969', '9093724', '17179098']
Number of rows after removing 'Error' or empty string values: 46
Mean difference for 'benefit_answer': 1.695652173913044
Column 'rigor_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'rigor_answer': ['17134892', '10637238', '16504757', '17264336', '16314619', '21399726', '22112969', '18794551', '18955454', '20564068', '11261827', '12177098', '20973267', '21060024', '20673585', '20153039', '20087643', '17179098', '21471562']
Number of rows after removing 'Error' or empty string values: 22
Mean difference for 'rigor_answer': 0.2727272727272725
Column 'importance_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'importance_answer': ['21060024', '20530276', '16148021', '20673585', '16504757', '18794551', '18955454', '21471562', '19273714', '12177098']
Number of rows after removing 'Error' or empty string values: 40
Mean difference for 'importance_answer': 1.2000000000000002
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['10547391', '9093724', '20800381', '16148021']
Number of rows after removing 'Error' or empty string values: 52
Mean difference for 'full_text_answer': 1.0
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['10547391', '21060024']
Number of rows after removing 'Error' or empty string values: 56
Mean difference for 'another_trial_answer': 1.6785714285714288

Overall mean difference across all answers: 1.169390175042349
Arguments Provided for the Evaluator:
Model:        openbiollm-8B
Output Path:  code/eval_outputs/openbiollm-8B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}

Saving outputs from model - openbiollm-8B to csv and json
Model outputs saved to code/eval_outputs/openbiollm-8B/openbiollm-8B_interpretation_outputs.json and code/eval_outputs/openbiollm-8B/openbiollm-8B_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['16314619', '20530276', '21471562', '22112969', '18794551', '17134892', '20153039', '12177098', '19273714', '20087643', '20564068']
Number of rows after removing 'Error' or empty string values: 38
Mean difference for 'benefit_answer': 2.7368421052631575
Column 'rigor_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'rigor_answer': ['20448107', '10547391']
Number of rows after removing 'Error' or empty string values: 56
Mean difference for 'rigor_answer': 0.2142857142857144
Column 'importance_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'importance_answer': ['17173959', '11261827', '17264336', '16504757', '20153039', '16314619', '20564068']
Number of rows after removing 'Error' or empty string values: 46
Mean difference for 'importance_answer': 2.086956521739131
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['9093724', '21399726', '20800381', '16314619', '20564068']
Number of rows after removing 'Error' or empty string values: 50
Mean difference for 'full_text_answer': 2.84
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['21399726', '10637238', '20673585', '21041710', '19273714', '20564068', '9093724', '11261827', '17173959', '16504757', '12177098', '21471562', '22112969', '16148021', '20800381', '18794551', '20087643', '17264336', '10547391', '18955454', '17134892', '20153039', '20530276']
Number of rows after removing 'Error' or empty string values: 14
Mean difference for 'another_trial_answer': 0.8571428571428577

Overall mean difference across all answers: 1.747045439686172
Arguments Provided for the Evaluator:
Model:        llama2_chat-13B
Output Path:  code/eval_outputs/llama2_chat-13B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}

Saving outputs from model - llama2_chat-13B to csv and json
Model outputs saved to code/eval_outputs/llama2_chat-13B/llama2_chat-13B_interpretation_outputs.json and code/eval_outputs/llama2_chat-13B/llama2_chat-13B_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['20153039', '20087643', '11261827', '16314619', '9093724', '17264336', '21471562', '20800381', '17134892', '17179098', '21399726', '16148021', '12177098', '17173959', '21060024', '22112969', '19273714', '17530429', '18794551', '15947110', '20673585', '21041710', '16504757', '20564068', '18955454', '20530276', '20973267', '10637238', '10547391', '20448107']
Number of rows after removing 'Error' or empty string values: 0
Mean difference for 'benefit_answer': nan
Column 'rigor_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'rigor_answer': ['20153039', '20087643', '11261827', '16314619', '9093724', '17264336', '21471562', '20800381', '17134892', '17179098', '21399726', '16148021', '12177098', '17173959', '21060024', '22112969', '19273714', '17530429', '18794551', '15947110', '20673585', '21041710', '16504757', '20564068', '18955454', '20530276', '20973267', '10637238', '10547391', '20448107']
Number of rows after removing 'Error' or empty string values: 0
Mean difference for 'rigor_answer': nan
Column 'importance_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'importance_answer': ['20153039', '20087643', '11261827', '16314619', '9093724', '17264336', '21471562', '20800381', '17134892', '17179098', '21399726', '16148021', '12177098', '17173959', '21060024', '22112969', '19273714', '17530429', '18794551', '15947110', '20673585', '21041710', '16504757', '20564068', '18955454', '20530276', '20973267', '10637238', '10547391', '20448107']
Number of rows after removing 'Error' or empty string values: 0
Mean difference for 'importance_answer': nan
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['20153039', '20087643', '11261827', '16314619', '9093724', '17264336', '21471562', '20800381', '17134892', '17179098', '21399726', '16148021', '12177098', '17173959', '21060024', '22112969', '19273714', '17530429', '18794551', '15947110', '20673585', '21041710', '16504757', '20564068', '18955454', '20530276', '20973267', '10637238', '10547391', '20448107']
Number of rows after removing 'Error' or empty string values: 0
Mean difference for 'full_text_answer': nan
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['20153039', '20087643', '11261827', '16314619', '9093724', '17264336', '21471562', '20800381', '17134892', '17179098', '21399726', '16148021', '12177098', '17173959', '21060024', '22112969', '19273714', '17530429', '18794551', '15947110', '20673585', '21041710', '16504757', '20564068', '18955454', '20530276', '20973267', '10637238', '10547391', '20448107']
Number of rows after removing 'Error' or empty string values: 0
Mean difference for 'another_trial_answer': nan

Overall mean difference across all answers: nan
Arguments Provided for the Evaluator:
Model:        llama3_instruct-8B
Output Path:  code/eval_outputs/llama3_instruct-8B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}

Saving outputs from model - llama3_instruct-8B to csv and json
Model outputs saved to code/eval_outputs/llama3_instruct-8B/llama3_instruct-8B_interpretation_outputs.json and code/eval_outputs/llama3_instruct-8B/llama3_instruct-8B_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.2
Mean difference for 'rigor_answer': 0.16666666666666607
Mean difference for 'importance_answer': 0.8999999999999995
Mean difference for 'full_text_answer': 1.5
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['20564068', '21060024', '17173959', '21041710']
Number of rows after removing 'Error' or empty string values: 52
Mean difference for 'another_trial_answer': 2.4615384615384617

Overall mean difference across all answers: 1.4456410256410255
