{
    "gpt4o": {
        "benefit_answer_mean_diff": 1.9333333333333331,
        "rigor_answer_mean_diff": -0.13333333333333286,
        "importance_answer_mean_diff": 0.8333333333333339,
        "full_text_answer_mean_diff": 1.7999999999999998,
        "another_trial_answer_mean_diff": 2.5000000000000004,
        "overall_avg": 1.386666666666667
    },
    "gpt4o-mini": {
        "benefit_answer_mean_diff": 2.266666666666667,
        "rigor_answer_mean_diff": 0.13333333333333286,
        "importance_answer_mean_diff": 1.2000000000000002,
        "full_text_answer_mean_diff": 2.033333333333333,
        "another_trial_answer_mean_diff": 2.066666666666667,
        "overall_avg": 1.54
    },
    "gpt35": {
        "benefit_answer_mean_diff": 3.8,
        "rigor_answer_mean_diff": 0.36666666666666625,
        "importance_answer_mean_diff": 2.033333333333333,
        "full_text_answer_mean_diff": 2.033333333333333,
        "another_trial_answer_mean_diff": 3.3000000000000003,
        "overall_avg": 2.3066666666666666
    },
    "gemini_1.5_flash": {
        "benefit_answer_mean_diff": 1.2333333333333334,
        "rigor_answer_mean_diff": -0.9000000000000004,
        "importance_answer_mean_diff": 0.8000000000000003,
        "full_text_answer_mean_diff": 0.7666666666666666,
        "another_trial_answer_mean_diff": 1.4333333333333336,
        "overall_avg": 0.6666666666666667
    },
    "gemini_1.5_flash-8B": {
        "benefit_answer_mean_diff": 1.833333333333333,
        "rigor_answer_mean_diff": -1.0,
        "importance_answer_mean_diff": 0.33333333333333304,
        "full_text_answer_mean_diff": 0.8999999999999999,
        "another_trial_answer_mean_diff": 1.3333333333333335,
        "overall_avg": 0.6799999999999999
    },
    "claude_3.5-sonnet": {
        "benefit_answer_mean_diff": 1.6333333333333337,
        "rigor_answer_mean_diff": -0.6333333333333329,
        "importance_answer_mean_diff": -1.166666666666666,
        "full_text_answer_mean_diff": 3.0,
        "another_trial_answer_mean_diff": 1.9333333333333331,
        "overall_avg": 0.9533333333333336
    },
    "claude_3.5-haiku": {
        "benefit_answer_mean_diff": 2.233333333333333,
        "rigor_answer_mean_diff": -0.7000000000000002,
        "importance_answer_mean_diff": 0.8999999999999995,
        "full_text_answer_mean_diff": 1.166666666666667,
        "another_trial_answer_mean_diff": 1.9333333333333327,
        "overall_avg": 1.1066666666666665
    },
    "alpacare-7B": {
        "benefit_answer_mean_diff": 4.333333333333333,
        "rigor_answer_mean_diff": 0.13333333333333286,
        "importance_answer_mean_diff": 0.20000000000000107,
        "full_text_answer_mean_diff": 0.0,
        "another_trial_answer_mean_diff": 0.0,
        "overall_avg": 0.9333333333333333
    },
    "biomistral7B": {
        "benefit_answer_mean_diff": 1.7166666666666668,
        "rigor_answer_mean_diff": 0.016666666666667496,
        "importance_answer_mean_diff": 0.6538461538461533,
        "full_text_answer_mean_diff": 0.6071428571428559,
        "another_trial_answer_mean_diff": 0.6607142857142856,
        "overall_avg": 0.7310073260073258
    },
    "llama2_chat-7B": {
        "benefit_answer_mean_diff": 2.3,
        "rigor_answer_mean_diff": 0.16666666666666696,
        "importance_answer_mean_diff": 0.2999999999999998,
        "full_text_answer_mean_diff": 0.6333333333333337,
        "another_trial_answer_mean_diff": 1.3000000000000003,
        "overall_avg": 0.9400000000000002
    },
    "llama2_chat-13B": {
        "benefit_answer_mean_diff": 1.1,
        "rigor_answer_mean_diff": 0.033333333333333215,
        "importance_answer_mean_diff": 0.49999999999999956,
        "full_text_answer_mean_diff": 1.6666666666666665,
        "another_trial_answer_mean_diff": 1.8666666666666667,
        "overall_avg": 1.0333333333333332
    },
    "llama2_chat-70B": {
        "benefit_answer_mean_diff": 2.0,
        "rigor_answer_mean_diff": -0.16666666666666607,
        "importance_answer_mean_diff": 0.10000000000000053,
        "full_text_answer_mean_diff": 0.033333333333333215,
        "another_trial_answer_mean_diff": 0.3600000000000003,
        "overall_avg": 0.4653333333333336
    },
    "llama3_instruct-8B": {
        "benefit_answer_mean_diff": 1.833333333333333,
        "rigor_answer_mean_diff": -0.13333333333333286,
        "importance_answer_mean_diff": 0.5666666666666664,
        "full_text_answer_mean_diff": 0.7000000000000002,
        "another_trial_answer_mean_diff": 1.4333333333333336,
        "overall_avg": 0.8800000000000001
    },
    "llama3_instruct-70B": {
        "benefit_answer_mean_diff": 4.1,
        "rigor_answer_mean_diff": -0.2333333333333334,
        "importance_answer_mean_diff": 0.8333333333333339,
        "full_text_answer_mean_diff": 3.033333333333333,
        "another_trial_answer_mean_diff": 3.733333333333333,
        "overall_avg": 2.2933333333333334
    },
    "med42-8B": {
        "benefit_answer_mean_diff": 2.1333333333333333,
        "rigor_answer_mean_diff": 0.0,
        "importance_answer_mean_diff": 0.9000000000000004,
        "full_text_answer_mean_diff": 1.0,
        "another_trial_answer_mean_diff": 2.9,
        "overall_avg": 1.3866666666666667
    },
    "med42-70B": {
        "benefit_answer_mean_diff": 4.5,
        "rigor_answer_mean_diff": 0.09999999999999964,
        "importance_answer_mean_diff": 1.0666666666666664,
        "full_text_answer_mean_diff": 3.1,
        "another_trial_answer_mean_diff": 4.333333333333333,
        "overall_avg": 2.6199999999999997
    },
    "olmo2_instruct-7B": {
        "benefit_answer_mean_diff": 3.3600000000000003,
        "rigor_answer_mean_diff": -0.9615384615384617,
        "importance_answer_mean_diff": 1.5,
        "full_text_answer_mean_diff": 2.0999999999999996,
        "another_trial_answer_mean_diff": 2.3666666666666663,
        "overall_avg": 1.673025641025641
    },
    "olmo2_instruct-13B": {
        "benefit_answer_mean_diff": 4.633333333333333,
        "rigor_answer_mean_diff": -0.2333333333333334,
        "importance_answer_mean_diff": 1.3666666666666663,
        "full_text_answer_mean_diff": 1.9000000000000004,
        "another_trial_answer_mean_diff": 2.7666666666666666,
        "overall_avg": 2.0866666666666664
    },
    "openbiollm-8B": {
        "benefit_answer_mean_diff": 3.304347826086956,
        "rigor_answer_mean_diff": -0.06896551724137989,
        "importance_answer_mean_diff": 1.5185185185185182,
        "full_text_answer_mean_diff": 2.518518518518519,
        "another_trial_answer_mean_diff": 1.7999999999999998,
        "overall_avg": 1.8144838691765226
    },
    "openbiollm-70B": {
        "benefit_answer_mean_diff": 3.833333333333333,
        "rigor_answer_mean_diff": -0.18333333333333268,
        "importance_answer_mean_diff": 1.0,
        "full_text_answer_mean_diff": 0.6153846153846159,
        "another_trial_answer_mean_diff": 4.7333333333333325,
        "overall_avg": 1.9997435897435898
    },
    "mistral_instruct7B": {
        "benefit_answer_mean_diff": 2.8666666666666663,
        "rigor_answer_mean_diff": -0.13333333333333286,
        "importance_answer_mean_diff": 1.5333333333333332,
        "full_text_answer_mean_diff": 2.6000000000000005,
        "another_trial_answer_mean_diff": 2.333333333333333,
        "overall_avg": 1.84
    },
    "biomedgpt7B": {
        "benefit_answer_mean_diff": 0.0,
        "rigor_answer_mean_diff": -1.0,
        "importance_answer_mean_diff": 0.0,
        "full_text_answer_mean_diff": -2.333333333333333,
        "another_trial_answer_mean_diff": null,
        "overall_avg": null
    }
}