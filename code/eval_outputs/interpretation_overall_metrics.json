{
    "gpt4o": {
        "benefit_answer_mean_diff": 3.1333333333333333,
        "rigor_answer_mean_diff": 0.1,
        "importance_answer_mean_diff": 1.2333333333333334,
        "full_text_answer_mean_diff": 2.8666666666666667,
        "another_trial_answer_mean_diff": 3.3333333333333335,
        "overall_mean_diff_avg": 2.1333333333333333
    },
    "gpt4o-mini":{
        "benefit_answer_mean_diff": 3.566666666666667,
        "rigor_answer_mean_diff": 1.4666666666666666,
        "importance_answer_mean_diff": 2.7333333333333334,
        "full_text_answer_mean_diff": 3.933333333333333,
        "another_trial_answer_mean_diff": 3.8666666666666667,
        "overall_mean_diff_avg": 3.1133333333333333
    },
    "gpt35": {
        "benefit_answer_mean_diff": 3.9,
        "rigor_answer_mean_diff": 1.4333333333333333,
        "importance_answer_mean_diff": 2.066666666666667,
        "full_text_answer_mean_diff": 2.6,
        "another_trial_answer_mean_diff": 3.7666666666666666,
        "overall_mean_diff_avg": 2.7533333333333334
    },
    "gemini_1.5_flash": {
        "benefit_answer_mean_diff": 2.5,
        "rigor_answer_mean_diff": -0.1,
        "importance_answer_mean_diff": 2.1666666666666665,
        "full_text_answer_mean_diff": 3,
        "another_trial_answer_mean_diff": 3.7,
        "overall_mean_diff_avg": 2.2533333333333334
    },
    "gemini_1.5_flash-8B": {
        "benefit_answer_mean_diff": 3.066666666666667,
        "rigor_answer_mean_diff": -0.1,
        "importance_answer_mean_diff": 0.9666666666666667,
        "full_text_answer_mean_diff": 2.7333333333333334,
        "another_trial_answer_mean_diff": 3.433333333333333,
        "overall_mean_diff_avg": 2.02
    },
    "claude_3.5-sonnet": {
        "benefit_answer_mean_diff": 2.4999999999999996,
        "rigor_answer_mean_diff": -0.16666666666666696,
        "importance_answer_mean_diff": -0.6333333333333337,
        "full_text_answer_mean_diff": 3.2333333333333334,
        "another_trial_answer_mean_diff": 2.8666666666666663,
        "overall_mean_diff_avg": 1.5599999999999996
    },
    "claude_3.5-haiku": {
        "benefit_answer_mean_diff": 2.9666666666666663,
        "rigor_answer_mean_diff": -0.033333333333333215,
        "importance_answer_mean_diff": 0.4666666666666668,
        "full_text_answer_mean_diff": 1.2999999999999998,
        "another_trial_answer_mean_diff": 2.166666666666667,
        "overall_mean_diff_avg": 1.3733333333333333
    },
    "alpacare-7B": {
        "benefit_answer_mean_diff": 6.051724137931034,
        "rigor_answer_mean_diff": 0.2666666666666675,
        "importance_answer_mean_diff": 0.8000000000000007,
        "full_text_answer_mean_diff": 0.0,
        "another_trial_answer_mean_diff": null,
        "overall_mean_diff_avg": null
    },
    "biomistral7B": {
        "benefit_answer_mean_diff": 1.666666666666667,
        "rigor_answer_mean_diff": 0.35000000000000053,
        "importance_answer_mean_diff": 1.1166666666666671,
        "full_text_answer_mean_diff": 1.0357142857142856,
        "another_trial_answer_mean_diff": 1.666666666666667,
        "overall_mean_diff_avg": 1.1671428571428575
    },
    "llama2_chat-7B": {
        "benefit_answer_mean_diff": 3.5,
        "rigor_answer_mean_diff": 0.5,
        "importance_answer_mean_diff": 1.0666666666666664,
        "full_text_answer_mean_diff": 0.33333333333333304,
        "another_trial_answer_mean_diff": 1.7000000000000002,
        "overall_mean_diff_avg": 1.42
    },
    "llama2_chat-13B": {
        "benefit_answer_mean_diff": 2.9333333333333336,
        "rigor_answer_mean_diff": 0.833333333333333,
        "importance_answer_mean_diff": 2.033333333333333,
        "full_text_answer_mean_diff": 2.366666666666667,
        "another_trial_answer_mean_diff": 2.7666666666666666,
        "overall_mean_diff_avg": 2.1866666666666665
    },
    "llama2_chat-70B": {
        "benefit_answer_mean_diff": 2.6896551724137927,
        "rigor_answer_mean_diff": 0.0,
        "importance_answer_mean_diff": 0.666666666666667,
        "full_text_answer_mean_diff": 0.6333333333333337,
        "another_trial_answer_mean_diff": 1.0333333333333332,
        "overall_mean_diff_avg": 1.0045977011494254
    },
    "llama3_instruct-8B": {
        "benefit_answer_mean_diff": 2.2,
        "rigor_answer_mean_diff": 0.16666666666666607,
        "importance_answer_mean_diff": 0.8999999999999995,
        "full_text_answer_mean_diff": 1.5,
        "another_trial_answer_mean_diff": 2.7333333333333334,
        "overall_mean_diff_avg": 1.4999999999999998
    },
    "llama3_instruct-70B": {
        "benefit_answer_mean_diff": 4.3999999999999995,
        "rigor_answer_mean_diff": -0.033333333333333215,
        "importance_answer_mean_diff": 0.9666666666666668,
        "full_text_answer_mean_diff": 3.3999999999999995,
        "another_trial_answer_mean_diff": 4.066666666666666,
        "overall_mean_diff_avg": 2.5599999999999996
    },
    "med42-8B": {
        "benefit_answer_mean_diff": 2.966666666666667,
        "rigor_answer_mean_diff": 0.33333333333333304,
        "importance_answer_mean_diff": 1.5999999999999996,
        "full_text_answer_mean_diff": 1.6333333333333329,
        "another_trial_answer_mean_diff": 3.033333333333333,
        "overall_mean_diff_avg": 1.913333333333333
    },
    "med42-70B": {
        "benefit_answer_mean_diff": 4.833333333333333,
        "rigor_answer_mean_diff": 0.0,
        "importance_answer_mean_diff": 1.3666666666666663,
        "full_text_answer_mean_diff": 4.366666666666667,
        "another_trial_answer_mean_diff": 4.766666666666667,
        "overall_mean_diff_avg": 3.0666666666666664
    },
    "olmo2_instruct-7B": {
        "benefit_answer_mean_diff": 3.233333333333333,
        "rigor_answer_mean_diff": 0.4666666666666668,
        "importance_answer_mean_diff": 1.3666666666666671,
        "full_text_answer_mean_diff": 2.3,
        "another_trial_answer_mean_diff": 2.7,
        "overall_mean_diff_avg": 2.013333333333333
    },
    "olmo2_instruct-13B": {
        "benefit_answer_mean_diff": 6.0,
        "rigor_answer_mean_diff": 0.40000000000000036,
        "importance_answer_mean_diff": 2.2333333333333325,
        "full_text_answer_mean_diff": 5.466666666666667,
        "another_trial_answer_mean_diff": 3.333333333333333,
        "overall_mean_diff_avg": 3.4866666666666664
    },
    "openbiollm-8B": {
        "benefit_answer_mean_diff": 2.7368421052631575,
        "rigor_answer_mean_diff": 0.2142857142857144,
        "importance_answer_mean_diff": 1.5925925925925926,
        "full_text_answer_mean_diff": 2.4399999999999995,
        "another_trial_answer_mean_diff": 0.75,
        "overall_mean_diff_avg": 1.5467440824282928
    },
    "openbiollm-70B": {
        "benefit_answer_mean_diff": 4.4,
        "rigor_answer_mean_diff": 0.0,
        "importance_answer_mean_diff": 0.6333333333333337,
        "full_text_answer_mean_diff": 1.0555555555555554,
        "another_trial_answer_mean_diff": 4.933333333333334,
        "overall_mean_diff_avg": 2.2044444444444444
    },
    "mistral_instruct7B": {
        "benefit_answer_mean_diff": 3.9000000000000004,
        "rigor_answer_mean_diff": 0.0,
        "importance_answer_mean_diff": 2.466666666666667,
        "full_text_answer_mean_diff": 2.466666666666667,
        "another_trial_answer_mean_diff": 2.633333333333333,
        "overall_mean_diff_avg": 2.2933333333333334
    },
    "biomedgpt7B": {
        "benefit_answer_mean_diff": 1.2407407407407405,
        "rigor_answer_mean_diff": 0.0,
        "importance_answer_mean_diff": 0.0,
        "full_text_answer_mean_diff": -0.1538461538461533,
        "another_trial_answer_mean_diff": null,
        "overall_mean_diff_avg": null
    }
}