{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for Calculating Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: the `json` files that contain overall metrics and also the linear regression model results were created manually.\n",
    "This can be done with some Python code very easily. However, the code for this is not included in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import altair as alt\n",
    "from utils import save_dataset_to_csv\n",
    "import statsmodels.formula.api as smf\n",
    "from tqdm import tqdm\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata = {\n",
    "    \"alpacare-7B\": {\"model_type\": \"biomedical open\", \"model_size_in_b\": 7},\n",
    "    \"biomedgpt7B\": {\"model_type\": \"biomedical open\", \"model_size_in_b\": 7},\n",
    "    \"biomistral7B\": {\"model_type\": \"biomedical open\", \"model_size_in_b\": 7},\n",
    "    \"claude_3.5-haiku\": {\"model_type\": \"generalist closed\", \"model_size_in_b\": None},\n",
    "    \"claude_3.5-sonnet\": {\"model_type\": \"generalist closed\", \"model_size_in_b\": None},\n",
    "    \"gemini_1.5_flash\": {\"model_type\": \"generalist closed\", \"model_size_in_b\": None},\n",
    "    \"gemini_1.5_flash-8B\": {\"model_type\": \"generalist closed\", \"model_size_in_b\": 8},\n",
    "    \"gpt4o\": {\"model_type\": \"generalist closed\", \"model_size_in_b\": None},\n",
    "    \"gpt4o-mini\": {\"model_type\": \"generalist closed\", \"model_size_in_b\": None},\n",
    "    \"gpt35\": {\"model_type\": \"generalist closed\", \"model_size_in_b\": 175},\n",
    "    \"llama2_chat-7B\": {\"model_type\": \"generalist open\", \"model_size_in_b\": 7},\n",
    "    \"llama2_chat-13B\": {\"model_type\": \"generalist open\", \"model_size_in_b\": 13},\n",
    "    \"llama2_chat-70B\": {\"model_type\": \"generalist open\", \"model_size_in_b\": 70},\n",
    "    \"llama3_instruct-8B\": {\"model_type\": \"generalist open\", \"model_size_in_b\": 8},\n",
    "    \"llama3_instruct-70B\": {\"model_type\": \"generalist open\", \"model_size_in_b\": 70},\n",
    "    \"med42-8B\": {\"model_type\": \"biomedical open\", \"model_size_in_b\": 8},\n",
    "    \"med42-70B\": {\"model_type\": \"biomedical open\", \"model_size_in_b\": 70},\n",
    "    \"mistral_instruct7B\": {\"model_type\": \"generalist open\", \"model_size_in_b\": 7},\n",
    "    \"olmo2_instruct-7B\": {\"model_type\": \"generalist open\", \"model_size_in_b\": 7},\n",
    "    \"olmo2_instruct-13B\": {\"model_type\": \"generalist open\", \"model_size_in_b\": 13},\n",
    "    \"openbiollm-8B\": {\"model_type\": \"biomedical open\", \"model_size_in_b\": 8},\n",
    "    \"openbiollm-70B\": {\"model_type\": \"biomedical open\", \"model_size_in_b\": 70}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin Detection Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection_overall_metrics.json was manually created\n",
    "detection_stats_df = pd.read_json(\"./eval_outputs/detection_overall_metrics.json\", orient=\"index\")\n",
    "\n",
    "detection_stats_df[\"model_name\"] = detection_stats_df.index\n",
    "detection_stats_df[\"model_type\"] = detection_stats_df.index.map(lambda x: model_metadata[x][\"model_type\"])\n",
    "detection_stats_df[\"model_size_in_b\"] = detection_stats_df.index.map(lambda x: model_metadata[x][\"model_size_in_b\"])\n",
    "# remove index\n",
    "detection_stats_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Number of models: {len(detection_stats_df)}\")\n",
    "\n",
    "detection_stats_df.sort_index(inplace=True) # alphabetical order\n",
    "detection_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average of accuracy by model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model type and calculate mean accuracy and standard deviation\n",
    "accuracy_by_model_type = detection_stats_df.groupby('model_type')['accuracy'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "accuracy_by_model_type.columns = ['model_type', 'mean_accuracy', 'std_deviation']\n",
    "\n",
    "print(accuracy_by_model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for custom labels\n",
    "custom_labels = {\n",
    "    \"alpacare-7B\": \"AlpaCare 7B\",\n",
    "    \"biomedgpt7B\": \"BioMedGPT 7B\",\n",
    "    \"biomistral7B\": \"BioMistral 7B\",\n",
    "    \"claude_3.5-haiku\": \"Claude3.5 Haiku\",\n",
    "    \"claude_3.5-sonnet\": \"Claude3.5 Sonnet\", \n",
    "    \"gemini_1.5_flash\": \"Gemini1.5 Flash\",\n",
    "    \"gemini_1.5_flash-8B\": \"Gemini1.5 Flash 8B\",\n",
    "    \"gpt4o\": \"GPT4o\",\n",
    "    \"gpt4o-mini\": \"GPT4o Mini\",\n",
    "    \"gpt35\": \"GPT3.5\",\n",
    "    \"llama2_chat-7B\": \"Llama2 Chat 7B\",\n",
    "    \"llama2_chat-13B\": \"Llama2 Chat 13B\",\n",
    "    \"llama2_chat-70B\": \"Llama2 Chat 70B\",\n",
    "    \"llama3_instruct-8B\": \"Llama3 Instruct 8B\",\n",
    "    \"llama3_instruct-70B\": \"Llama3 Instruct 70B\",\n",
    "    \"med42-8B\": \"Med42 8B\",\n",
    "    \"med42-70B\": \"Med42 70B\",\n",
    "    \"mistral_instruct7B\": \"Mistral Instruct 7B\",\n",
    "    \"olmo2_instruct-7B\": \"Olmo2 Instruct 7B\",\n",
    "    \"olmo2_instruct-13B\": \"Olmo2 Instruct 13B\",\n",
    "    \"openbiollm-8B\": \"OpenBioLLM 8B\",\n",
    "    \"openbiollm-70B\": \"OpenBioLLM 70B\"\n",
    "}\n",
    "\n",
    "detection_stats_df['model_name_custom'] = detection_stats_df['model_name'].map(custom_labels)\n",
    "\n",
    "color_mapping = {\n",
    "    'biomedical open': '#0868ac', \n",
    "    'generalist closed': '#7bccc4',\n",
    "    'generalist open': '#bae4bc',\n",
    "}\n",
    "\n",
    "# Create the bar chart\n",
    "chart = alt.Chart(detection_stats_df).mark_bar().encode(\n",
    "    y=alt.Y('model_name_custom:N', sort='-x', title='Model Name'),\n",
    "    x=alt.X('accuracy:Q', title='Accuracy'),\n",
    "    color=alt.Color('model_type:N', title='Model Type',\n",
    "                    scale=alt.Scale(domain=list(color_mapping.keys()), range=list(color_mapping.values())),\n",
    "                    legend=alt.Legend(\n",
    "                    orient='none',\n",
    "                    legendX=130, legendY=-45,\n",
    "                    direction='horizontal',\n",
    "                    titleAnchor='middle'))  # Legend at the bottom\n",
    ").properties(\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "# Add value labels with increased font size\n",
    "text = chart.mark_text(\n",
    "    align='center',\n",
    "    baseline='middle',\n",
    "    fontWeight='bold',\n",
    "    dx=20  # Adjust the position of the text\n",
    ").encode(\n",
    "    text=alt.Text('accuracy:Q', format='.2f'),\n",
    "    color=alt.value('black'),\n",
    ")\n",
    "\n",
    "# Add a mean rule\n",
    "avg_rule = alt.Chart(detection_stats_df).mark_rule(color='red').encode(\n",
    "    x='mean(accuracy):Q',\n",
    "    size=alt.value(2)\n",
    ")\n",
    "\n",
    "# Add a 50% chance rule\n",
    "chance_rule = alt.Chart(detection_stats_df).mark_rule(color='gray').encode(\n",
    "    x='min(accuracy):Q',\n",
    "    size=alt.value(2),\n",
    "    strokeDash=alt.value([10, 10])\n",
    ")\n",
    "\n",
    "# Increase font size for axis labels, titles, and other components\n",
    "chart_config = {\n",
    "    \"axis\": {\"labelFontSize\": 20, \"titleFontSize\": 22},  # Axis labels and titles\n",
    "    \"header\": {\"labelFontSize\": 20, \"titleFontSize\": 22},  # Title and facet headers (if any)\n",
    "    \"legend\": {\"labelFontSize\": 18, \"titleFontSize\": 20},  # Legend labels and titles\n",
    "    \"text\": {\"fontSize\": 20},  # Text mark size\n",
    "}\n",
    "\n",
    "# Combine chart and text, and apply the config\n",
    "c_t = chart + avg_rule + chance_rule + text\n",
    "c_t = c_t.configure(**chart_config)  # Apply the global configuration\n",
    "\n",
    "# Save to HTML\n",
    "c_t.save(\"./plots/detection_accuracy_by_model.html\")\n",
    "\n",
    "# Display the chart\n",
    "c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average accuracy by model_type and add error bars\n",
    "bars = alt.Chart(detection_stats_df).mark_bar().encode(\n",
    "    x=alt.X('model_type:N', title='Model Type', axis=alt.Axis(labelAngle=0)),\n",
    "    y=alt.Y('mean(accuracy):Q', title='Mean Accuracy'),\n",
    "    color=alt.Color('model_type:N', title='Model Type', legend=None)\n",
    ").properties(\n",
    "    title='Average Accuracy by Model Type',\n",
    "    width=800  # Set the width to 800 pixels\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart(detection_stats_df).mark_errorbar(extent='stdev').encode(\n",
    "    x=alt.X('model_type:N'),\n",
    "    y=alt.Y('accuracy:Q')\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "text = bars.mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    dy=-5  # Adjust the position of the text\n",
    ").encode(\n",
    "    text=alt.Text('mean(accuracy):Q', format='.2f')\n",
    ")\n",
    "\n",
    "alt.layer(bars, error_bars, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average of accuracy by model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model size in buckets (0-10B, 11-20B, 22-100B, 100B+/NaN)\n",
    "def model_size_bucket(model_size): \n",
    "    if model_size is None or pd.isna(model_size):\n",
    "        return \"Unknown\"\n",
    "    elif model_size >= 100:\n",
    "        return \"100B+\"\n",
    "    elif model_size <= 10:\n",
    "        return \"0-10B\"\n",
    "    elif model_size <= 20:\n",
    "        return \"11-20B\"\n",
    "    else:\n",
    "        return \"21-100B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average accuracy by model size\n",
    "detection_stats_df[\"model_size_bucket\"] = detection_stats_df[\"model_size_in_b\"].map(model_size_bucket)\n",
    "\n",
    "accuracy_by_model_size = detection_stats_df.groupby('model_size_bucket')['accuracy'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "accuracy_by_model_size.columns = ['model_size_bucket', 'mean_accuracy', 'std_deviation']\n",
    "\n",
    "print(accuracy_by_model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = alt.Chart(detection_stats_df).mark_bar().encode(\n",
    "    x=alt.X('model_name:N', sort='-y', title='Model Name'),\n",
    "    y=alt.Y('accuracy:Q', title='Accuracy'),\n",
    "    color=alt.Color('model_size_bucket:N', title='Model Size Bucket')\n",
    ").properties(\n",
    "    title='Accuracy by Model Size Bucket',\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "text = bars.mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    dy=-5  # Adjust the position of the text\n",
    ").encode(\n",
    "    text=alt.Text('accuracy:Q', format='.2f')\n",
    ")\n",
    "\n",
    "bars + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCT Trial Result Interpretation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretation_overall_metrics.json was manually created\n",
    "interpretation_stats_df = pd.read_json(\"./eval_outputs/interpretation_overall_metrics.json\", orient=\"index\")\n",
    "\n",
    "interpretation_stats_df[\"model_name\"] = interpretation_stats_df.index\n",
    "interpretation_stats_df[\"model_type\"] = interpretation_stats_df.index.map(lambda x: model_metadata[x][\"model_type\"])\n",
    "interpretation_stats_df[\"model_size_in_b\"] = interpretation_stats_df.index.map(lambda x: model_metadata[x][\"model_size_in_b\"])\n",
    "# remove index\n",
    "interpretation_stats_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Number of models: {len(interpretation_stats_df)}\")\n",
    "\n",
    "interpretation_stats_df.sort_index(inplace=True) # alphabetical order\n",
    "interpretation_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_expert_stats = {\n",
    "        \"benefit_answer\": {\"mean_diff\": 0.71, \"ci_lower\": 0.07, \"ci_upper\": 1.35},\n",
    "        \"rigor_answer\": {\"mean_diff\": -0.59, \"ci_lower\": -1.13, \"ci_upper\": -0.05},\n",
    "        \"importance_answer\": {\"mean_diff\": -0.38, \"ci_lower\": -0.95, \"ci_upper\": 0.19},\n",
    "        \"full_text_answer\": {\"mean_diff\": 0.77, \"ci_lower\": 0.08, \"ci_upper\": 1.47},\n",
    "        \"another_trial_answer\": {\"mean_diff\": 0.64, \"ci_lower\": -0.03, \"ci_upper\": 1.31}\n",
    "    }\n",
    "\n",
    "human_expert_stats_df = pd.DataFrame(human_expert_stats).T\n",
    "human_expert_stats_df[\"metric\"] = human_expert_stats_df.index\n",
    "# remove index\n",
    "human_expert_stats_df.reset_index(drop=True, inplace=True)\n",
    "human_expert_stats_df[\"method\"] = \"human experts\"\n",
    "\n",
    "human_expert_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_interval(df, df_column_name):\n",
    "    mean_diff = df[df_column_name].mean()  # Calculate the mean\n",
    "    std_dev = df[df_column_name].std()  # Calculate the standard deviation\n",
    "    n = len(df[df_column_name])  # Sample size\n",
    "\n",
    "    # Calculate the margin of error for 95% CI (z = 1.96)\n",
    "    z = 1.96\n",
    "    margin_of_error = z * (std_dev / sqrt(n))\n",
    "\n",
    "    # Calculate the 95% Confidence Interval\n",
    "    ci_lower = mean_diff - margin_of_error\n",
    "    ci_upper = mean_diff + margin_of_error\n",
    "\n",
    "    return ci_lower, ci_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_stats(interpretation_stats_df, method_name = \"all LLMs\"):\n",
    "    # calculate the average of all model metrics and calculate 95% CI\n",
    "    average_model_benefit = interpretation_stats_df[\"benefit_answer_mean_diff\"].mean()\n",
    "    ci_lower_model_benefit, ci_upper_model_benefit = calculate_confidence_interval(interpretation_stats_df, \"benefit_answer_mean_diff\")\n",
    "\n",
    "    average_model_rigor = interpretation_stats_df[\"rigor_answer_mean_diff\"].mean()\n",
    "    ci_lower_model_rigor, ci_upper_model_rigor = calculate_confidence_interval(interpretation_stats_df, \"rigor_answer_mean_diff\")\n",
    "\n",
    "    average_model_importance = interpretation_stats_df[\"importance_answer_mean_diff\"].mean()\n",
    "    ci_lower_model_importance, ci_upper_model_importance = calculate_confidence_interval(interpretation_stats_df, \"importance_answer_mean_diff\")\n",
    "\n",
    "    average_model_full_text = interpretation_stats_df[\"full_text_answer_mean_diff\"].mean()\n",
    "    ci_lower_model_full_text, ci_upper_model_full_text = calculate_confidence_interval(interpretation_stats_df, \"full_text_answer_mean_diff\")\n",
    "\n",
    "    average_model_another_trial = interpretation_stats_df[\"another_trial_answer_mean_diff\"].mean()\n",
    "    ci_lower_model_another_trial, ci_upper_model_another_trial = calculate_confidence_interval(interpretation_stats_df, \"another_trial_answer_mean_diff\")\n",
    "\n",
    "    model_stats = {\n",
    "        \"benefit_answer\": {\"mean_diff\": average_model_benefit, \"ci_lower\": ci_lower_model_benefit, \"ci_upper\": ci_upper_model_benefit},\n",
    "        \"rigor_answer\": {\"mean_diff\": average_model_rigor, \"ci_lower\": ci_lower_model_rigor, \"ci_upper\": ci_upper_model_rigor},\n",
    "        \"importance_answer\": {\"mean_diff\": average_model_importance, \"ci_lower\": ci_lower_model_importance, \"ci_upper\": ci_upper_model_importance},\n",
    "        \"full_text_answer\": {\"mean_diff\": average_model_full_text, \"ci_lower\": ci_lower_model_full_text, \"ci_upper\": ci_upper_model_full_text},\n",
    "        \"another_trial_answer\": {\"mean_diff\": average_model_another_trial, \"ci_lower\": ci_lower_model_another_trial, \"ci_upper\": ci_upper_model_another_trial}\n",
    "    }\n",
    "\n",
    "    model_stats_df = pd.DataFrame(model_stats).T\n",
    "    model_stats_df[\"metric\"] = model_stats_df.index\n",
    "    # remove index\n",
    "    model_stats_df.reset_index(drop=True, inplace=True)\n",
    "    model_stats_df[\"method\"] = method_name\n",
    "\n",
    "    return model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average of all model metrics and calculate 95% CI\n",
    "model_stats_df = calculate_model_stats(interpretation_stats_df)\n",
    "\n",
    "model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average and 95% CI by model_type from interpretation_stats_df for each model_type\n",
    "interpretation_stats_df[\"model_type\"] = interpretation_stats_df[\"model_name\"].map(lambda x: model_metadata[x][\"model_type\"])\n",
    "\n",
    "# get only generalist closed  models\n",
    "generalist_closed_model_stats = interpretation_stats_df[interpretation_stats_df[\"model_type\"] == \"generalist closed\"]\n",
    "\n",
    "# get only generalist open models\n",
    "generalist_open_model_stats = interpretation_stats_df[interpretation_stats_df[\"model_type\"] == \"generalist open\"]\n",
    "\n",
    "# get only biomedical open models\n",
    "biomedical_open_model_stats = interpretation_stats_df[interpretation_stats_df[\"model_type\"] == \"biomedical open\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalist_closed_model_stats_df = calculate_model_stats(generalist_closed_model_stats, method_name = \"generalist closed\")\n",
    "\n",
    "generalist_open_model_stats_df = calculate_model_stats(generalist_open_model_stats, method_name = \"generalist open\")\n",
    "\n",
    "biomedical_open_model_stats_df = calculate_model_stats(biomedical_open_model_stats, method_name = \"biomedical open\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalist_closed_model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalist_open_model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomedical_open_model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_by_model_type = pd.concat([generalist_closed_model_stats_df, generalist_open_model_stats_df, biomedical_open_model_stats_df], ignore_index=True)\n",
    "\n",
    "average_by_model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all the dataframes\n",
    "model_stats_final_df = pd.concat([human_expert_stats_df, model_stats_df, average_by_model_type], ignore_index=True)\n",
    "#drop \"_answer\" from the values in metric column\n",
    "model_stats_final_df['metric'] = model_stats_final_df['metric'].str.replace('_answer', '')\n",
    "\n",
    "model_stats_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for custom facet titles\n",
    "facet_title_mapping = {\n",
    "    'benefit': 'Treatment Benefit',\n",
    "    'rigor': 'Study Rigor',\n",
    "    'importance': 'Study Importance',\n",
    "    'full_text': 'Interest to Read Full-Text',\n",
    "    'another_trial': 'Interest to Run Another Trial'\n",
    "}\n",
    "\n",
    "# Define the desired order for the facets\n",
    "facet_order = ['Treatment Benefit', 'Study Rigor', 'Study Importance', 'Interest to Read Full-Text', 'Interest to Run Another Trial']\n",
    "\n",
    "color_mapping = {\n",
    "    'human experts': '#0868ac', \n",
    "    'all LLMs': '#43a2ca',  \n",
    "    'generalist closed': '#7bccc4',  \n",
    "    'generalist open': '#bae4bc',  \n",
    "    'biomedical open': '#E3F4D4'\n",
    "}\n",
    "\n",
    "method_order = ['human experts', 'all LLMs', 'generalist closed', 'generalist open', 'biomedical open']\n",
    "\n",
    "# Apply the mapping as a calculated field\n",
    "chart_data = model_stats_final_df.copy()\n",
    "chart_data['metric'] = chart_data['metric'].map(facet_title_mapping)\n",
    "\n",
    "# Configure global font sizes\n",
    "chart_config = {\n",
    "    \"axis\": {\"labelFontSize\": 20, \"titleFontSize\": 22},  # Axis labels and titles\n",
    "    \"header\": {\"labelFontSize\": 20, \"titleFontSize\": 22},  # Facet headers\n",
    "    \"legend\": {\"labelFontSize\": 18, \"titleFontSize\": 20},  # Legend labels and titles\n",
    "    \"text\": {\"fontSize\": 20},  # Text mark size\n",
    "}\n",
    "\n",
    "# Bar chart\n",
    "bars = alt.Chart(chart_data).mark_bar().encode(\n",
    "    x=alt.X('method:N', title=None, axis=alt.Axis(labelAngle=-45), sort=method_order),\n",
    "    y=alt.Y('mean_diff:Q', title='Mean Difference'),\n",
    "    color=alt.Color('method:N', title='Method', legend=None, scale=alt.Scale(domain=list(color_mapping.keys()), range=list(color_mapping.values())))\n",
    ").properties(\n",
    "    width=300,  # Set the width to 300 pixels\n",
    "    height=300  # Set the height to 300 pixels\n",
    ")\n",
    "\n",
    "# Error bars\n",
    "error_bars = alt.Chart(chart_data).mark_errorbar().encode(\n",
    "    alt.X(\"method:N\", sort=method_order),\n",
    "    alt.Y(\"ci_lower:Q\").title(\"Mean Difference\"),\n",
    "    alt.Y2(\"ci_upper:Q\"),\n",
    "    strokeWidth=alt.value(2),\n",
    "    color=alt.value('gray')\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "text = bars.mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    fontWeight='bold',\n",
    "    dy=alt.expr(expr=alt.expr.if_(alt.datum.mean_diff >= 0, -1, 20))  # Adjust the position of the text    \n",
    ").encode(\n",
    "    text=alt.Text('mean_diff:Q', format='.2f'),\n",
    "    color=alt.value('black')  # Set text color to black\n",
    ")\n",
    "\n",
    "# Combine layers and facet\n",
    "chart = alt.layer(bars, error_bars, text, data=chart_data).facet(\n",
    "    column=alt.Column('metric:N', title=None, sort=facet_order),\n",
    ").configure(**chart_config)  # Apply the global configuration\n",
    "\n",
    "# save to html\n",
    "chart.save(\"./plots/interpretation_by_measures.html\")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitigation Strategies Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats_df['method'] = 'baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold_labelled_interpretation_overall_metrics.json was manually created\n",
    "gold_labelled_interpretation_stats_df = pd.read_json(\"./eval_outputs/gold_labelled_interpretation_overall_metrics.json\", orient=\"index\")\n",
    "\n",
    "gold_labelled_interpretation_stats_df[\"model_name\"] = gold_labelled_interpretation_stats_df.index\n",
    "gold_labelled_interpretation_stats_df[\"model_type\"] = gold_labelled_interpretation_stats_df.index.map(lambda x: model_metadata[x][\"model_type\"])\n",
    "gold_labelled_interpretation_stats_df[\"model_size_in_b\"] = gold_labelled_interpretation_stats_df.index.map(lambda x: model_metadata[x][\"model_size_in_b\"])\n",
    "# remove index\n",
    "gold_labelled_interpretation_stats_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Number of models: {len(gold_labelled_interpretation_stats_df)}\")\n",
    "\n",
    "gold_labelled_interpretation_stats_df.sort_index(inplace=True) # alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average of all model metrics and calculate 95% CI\n",
    "gold_labelled_model_stats_df = calculate_model_stats(gold_labelled_interpretation_stats_df, method_name = \"+ ref labels\")\n",
    "gold_labelled_model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_output_labelled_interpretation_overall_metrics.json was manually created\n",
    "model_output_labelled_interpretation_stats_df = pd.read_json(\"./eval_outputs/model_output_labelled_interpretation_overall_metrics.json\", orient=\"index\")\n",
    "\n",
    "model_output_labelled_interpretation_stats_df[\"model_name\"] = model_output_labelled_interpretation_stats_df.index\n",
    "model_output_labelled_interpretation_stats_df[\"model_type\"] = model_output_labelled_interpretation_stats_df.index.map(lambda x: model_metadata[x][\"model_type\"])\n",
    "model_output_labelled_interpretation_stats_df[\"model_size_in_b\"] = model_output_labelled_interpretation_stats_df.index.map(lambda x: model_metadata[x][\"model_size_in_b\"])\n",
    "# remove index\n",
    "model_output_labelled_interpretation_stats_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Number of models: {len(model_output_labelled_interpretation_stats_df)}\")\n",
    "\n",
    "model_output_labelled_interpretation_stats_df.sort_index(inplace=True) # alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average of all model metrics and calculate 95% CI\n",
    "model_output_labelled_model_stats_df = calculate_model_stats(model_output_labelled_interpretation_stats_df, method_name = \"+ model labels\")\n",
    "model_output_labelled_model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_detection_interpretation_overall_metrics.json was manually created\n",
    "combined_interpretation_stats_df = pd.read_json(\"./eval_outputs/combined_detection_interpretation_overall_metrics.json\", orient=\"index\")\n",
    "\n",
    "combined_interpretation_stats_df[\"model_name\"] = combined_interpretation_stats_df.index\n",
    "combined_interpretation_stats_df[\"model_type\"] = combined_interpretation_stats_df.index.map(lambda x: model_metadata[x][\"model_type\"])\n",
    "combined_interpretation_stats_df[\"model_size_in_b\"] = combined_interpretation_stats_df.index.map(lambda x: model_metadata[x][\"model_size_in_b\"])\n",
    "# remove index\n",
    "combined_interpretation_stats_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Number of models: {len(combined_interpretation_stats_df)}\")\n",
    "\n",
    "combined_interpretation_stats_df.sort_index(inplace=True) # alphabetical order\n",
    "# combined_interpretation_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average of all model metrics and calculate 95% CI\n",
    "combined_interpretation_stats_df = calculate_model_stats(combined_interpretation_stats_df, method_name = \"detect + interpret\")\n",
    "combined_interpretation_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([human_expert_stats_df, model_stats_df, gold_labelled_model_stats_df, model_output_labelled_model_stats_df, combined_interpretation_stats_df], ignore_index=True)\n",
    "\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for custom facet titles\n",
    "facet_title_mapping = {\n",
    "    'benefit_answer': 'Treatment Benefit',\n",
    "    'rigor_answer': 'Study Rigor',\n",
    "    'importance_answer': 'Study Importance',\n",
    "    'full_text_answer': 'Interest to Read Full-Text',\n",
    "    'another_trial_answer': 'Interest to Run Another Trial'\n",
    "}\n",
    "\n",
    "# Define the desired order for the facets\n",
    "facet_order = ['Treatment Benefit', 'Study Rigor', 'Study Importance', 'Interest to Read Full-Text', 'Interest to Run Another Trial']\n",
    "\n",
    "color_mapping = {\n",
    "    'human experts': '#0868ac',  \n",
    "    'baseline': '#43a2ca',  \n",
    "    '+ ref labels': '#7bccc4',  \n",
    "    '+ model labels': '#bae4bc', \n",
    "    'detect + interpret': '#E3F4D4'  \n",
    "}\n",
    "\n",
    "method_order = ['human experts', 'baseline', '+ ref labels', '+ model labels', 'detect + interpret']\n",
    "\n",
    "# Apply the mapping as a calculated field\n",
    "chart_data = all_results.copy()\n",
    "chart_data['metric'] = chart_data['metric'].map(facet_title_mapping)\n",
    "\n",
    "# Configure global font sizes\n",
    "chart_config = {\n",
    "    \"axis\": {\"labelFontSize\": 20, \"titleFontSize\": 22},  # Axis labels and titles\n",
    "    \"header\": {\"labelFontSize\": 20, \"titleFontSize\": 22},  # Facet headers\n",
    "    \"legend\": {\"labelFontSize\": 18, \"titleFontSize\": 20},  # Legend labels and titles\n",
    "    \"text\": {\"fontSize\": 20},  # Text mark size\n",
    "}\n",
    "\n",
    "# Bar chart\n",
    "bars = alt.Chart(chart_data).mark_bar().encode(\n",
    "    x=alt.X('method:N', title=None, axis=alt.Axis(labelAngle=-45), sort = method_order),\n",
    "    y=alt.Y('mean_diff:Q', title='Mean Difference'),\n",
    "    color=alt.Color('method:N', title='Method', legend=None, scale=alt.Scale(domain=list(color_mapping.keys()), range=list(color_mapping.values())))\n",
    ").properties(\n",
    "    width=300,  # Set the width to 300 pixels\n",
    "    height=300  # Set the height to 300 pixels\n",
    ")\n",
    "\n",
    "# Error bars\n",
    "error_bars = alt.Chart(chart_data).mark_errorbar().encode(\n",
    "    alt.X(\"method:N\", sort = method_order),\n",
    "    alt.Y(\"ci_lower:Q\").title(\"Mean Difference\"),\n",
    "    alt.Y2(\"ci_upper:Q\"),\n",
    "    strokeWidth=alt.value(2),\n",
    "    color=alt.value('gray')\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "text = bars.mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    fontWeight='bold',\n",
    "    dy=alt.expr(expr=alt.expr.if_(alt.datum.mean_diff >= 0, -1, 20))  # Adjust the position of the text    \n",
    ").encode(\n",
    "    text=alt.Text('mean_diff:Q', format='.2f'),\n",
    "    color=alt.value('black')  # Set text color to black\n",
    ")\n",
    "\n",
    "# Combine layers and facet\n",
    "chart = alt.layer(bars, error_bars, text, data=chart_data).facet(\n",
    "    column=alt.Column('metric:N', title=None, sort=facet_order),\n",
    ").configure(**chart_config)  # Apply the global configuration\n",
    "\n",
    "# save to html\n",
    "chart.save(\"./plots/interpretation_by_measures_all_methods.html\")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between spin detection and spin interpretation\n",
    "\n",
    "Linear Regression with statsmodels Python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all model names\n",
    "model_names = model_metadata.keys()\n",
    "# remove alpacare-13B\n",
    "model_names = [x for x in model_names if x != \"alpacare-13B\"]\n",
    "\n",
    "len(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = [\"benefit_answer\", \"rigor_answer\", \"importance_answer\", \"full_text_answer\", \"another_trial_answer\"]\n",
    "gpt_models = [\"gpt4o\", \"gpt4o-mini\", \"gpt35\"]\n",
    "huggingface_models = [\"alpacare-7B\", \"biomedgpt7B\", \"biomistral7B\", \n",
    "                      \"llama2_chat-7B\", \"llama2_chat-13B\", \"llama2_chat-70B\",\n",
    "                      \"llama3_instruct-8B\", \"llama3_instruct-70B\",\n",
    "                      \"med42-8B\", \"med42-70B\", \"mistral_instruct7B\", \n",
    "                      \"olmo2_instruct-7B\", \"olmo2_instruct-13B\",\n",
    "                      \"openbiollm-8B\", \"openbiollm-70B\"]\n",
    "no_probability_models = [\"claude_3.5-haiku\", \"claude_3.5-sonnet\", \"gemini_1.5_flash\", \"gemini_1.5_flash-8B\"]\n",
    "\n",
    "def get_is_detection_correct(row):\n",
    "    if row['abstract_type'] == \"spin\":\n",
    "        return row['model_answer'] == \"yes\"\n",
    "    else:\n",
    "        return row['model_answer'] == \"no\"\n",
    "    \n",
    "def get_is_abstract_type_spin(row):\n",
    "    return row['abstract_type'] == \"spin\"\n",
    "    \n",
    "def detection_probability_gpt(row):\n",
    "    # find the first instance of \"yes\" or \"no\"\n",
    "    token_probabilties = row['model_log_probabilities']\n",
    "    for token_prob in token_probabilties:\n",
    "        if token_prob['token'].lower() == \"yes\":\n",
    "            return np.exp(token_prob['logprob'])\n",
    "        elif token_prob['token'].lower() == \"no\":\n",
    "            return np.exp(token_prob['logprob'])\n",
    "    return None # this should not happen but just in case\n",
    "\n",
    "def detection_probability_huggingface(row):\n",
    "    # find the first instance of \"yes\" or \"no\"\n",
    "    token_probabilties = row['model_log_probabilities']\n",
    "    for token_prob in token_probabilties:\n",
    "        if token_prob['token_string'].lower() == \"yes\":\n",
    "            return token_prob['probability']\n",
    "        elif token_prob['token_string'].lower() == \"no\":\n",
    "            return token_prob['probability']\n",
    "    return None # this should not happen but just in case\n",
    "\n",
    "\n",
    "def prepare_data_for_regression(model_names):\n",
    "    for model_name in tqdm(model_names):\n",
    "        # print(f\"Processing {model_name}...\")\n",
    "        final_data = []\n",
    "        detection_output_file_path = f\"./eval_outputs/{model_name}/{model_name}_detection_outputs.json\"\n",
    "        interpretation_output_file_path = f\"./eval_outputs/{model_name}/{model_name}_interpretation_outputs.json\"\n",
    "        model_detection_data = pd.read_json(detection_output_file_path, orient=\"records\")\n",
    "        model_interpretation_data = pd.read_json(interpretation_output_file_path, orient=\"records\")\n",
    "\n",
    "        # merge model_detection_data and model_interpretation_data by PMID and abstract_type\n",
    "        model_data = pd.merge(model_detection_data, model_interpretation_data, on=['PMID', 'abstract_type'])\n",
    "\n",
    "        # loop through each row in model_data\n",
    "        for _, row in model_data.iterrows():\n",
    "            detection_model_prediction = 1 if row['model_answer'] == \"yes\" else 0\n",
    "            is_detection_correct = 1 if get_is_detection_correct(row) else 0\n",
    "            is_spin_in_abstract = 1 if get_is_abstract_type_spin(row) else 0\n",
    "\n",
    "            if model_name in gpt_models:\n",
    "                detection_probability = detection_probability_gpt(row)\n",
    "            elif model_name in huggingface_models:\n",
    "                detection_probability = detection_probability_huggingface(row)\n",
    "            else:\n",
    "                detection_probability = None\n",
    "            \n",
    "            for measure in measures:\n",
    "                final_data.append({\n",
    "                    \"pmid\": row['PMID'],\n",
    "                    \"measure\": measure,\n",
    "                    \"is_spin_in_abstract\": is_spin_in_abstract,\n",
    "                    \"is_detection_correct\": is_detection_correct,\n",
    "                    \"detection_model_prediction\": detection_model_prediction,\n",
    "                    \"detection_probability\": detection_probability,\n",
    "                    \"interpretation_answer\": float(row[measure]) if row[measure] != \"\" else None\n",
    "                })\n",
    "            # calculate the average of the differences\n",
    "            answers = []\n",
    "            for measure in measures:\n",
    "                if row[measure] != \"\":\n",
    "                    answers.append(float(row[measure]))\n",
    "            if len(answers) > 0:\n",
    "                avg_answer= round(np.mean(answers), 6)\n",
    "            else:\n",
    "                avg_answer = None\n",
    "            # add the average difference to the data\n",
    "            final_data.append({\n",
    "                \"pmid\": row['PMID'],\n",
    "                \"measure\": \"overall\",\n",
    "                \"is_spin_in_abstract\": is_spin_in_abstract,\n",
    "                \"is_detection_correct\": is_detection_correct,\n",
    "                \"detection_model_prediction\": detection_model_prediction,\n",
    "                \"detection_probability\": detection_probability,\n",
    "                \"interpretation_answer\": avg_answer\n",
    "            })\n",
    "\n",
    "        # save the final data to a json file\n",
    "        json_file_path = f\"./eval_outputs/{model_name}/{model_name}_combined_data.csv\"\n",
    "        save_dataset_to_csv(final_data, json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data_for_regression(model_names=model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplest Regression\n",
    "\n",
    "Is spin in abstract and the measures answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    output_string = \"\"\n",
    "    csv_file_path = f\"./eval_outputs/{model_name}/{model_name}_combined_data.csv\"\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    measures = [\"benefit_answer\", \"rigor_answer\", \"importance_answer\", \"full_text_answer\", \"another_trial_answer\", \"overall\"]\n",
    "    for measure in measures:\n",
    "        # get the data for the current measure\n",
    "        measure_data = data[data['measure'] == measure]\n",
    "        nan_rows_number = measure_data['interpretation_answer'].isnull().sum()\n",
    "        # remove rows with NaN values in interpretation_answer\n",
    "        measure_data = measure_data.dropna(subset=['interpretation_answer'])\n",
    "\n",
    "        # check if there are less than 2 rows\n",
    "        if len(measure_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        # fit the model\n",
    "        model = smf.ols(formula=\"interpretation_answer ~ is_spin_in_abstract\", \n",
    "                                    data=measure_data)\n",
    "        results = model.fit()\n",
    "\n",
    "        output_string += f\"Model: {model_name} - {measure}\\n\"\n",
    "        # print number of rows with NaN value(s)\n",
    "        output_string += f\"Number of rows with NaN value(s) in {model_name}: {nan_rows_number}\\n\"\n",
    "        output_string += results.summary().as_text()\n",
    "        output_string += \"\\n\"\n",
    "\n",
    "    # save the model summary\n",
    "    with open(f\"./eval_outputs/{model_name}/{model_name}_simple_regression_summary.txt\", \"w\") as f:\n",
    "        f.write(output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forest Plot for \"Benefit\" Linear Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON data into a DataFrame\n",
    "# this json file was manually created\n",
    "regression_results_df = pd.read_json(\"./eval_outputs/simple_linear_regression_benefit_data.json\", orient=\"index\")\n",
    "\n",
    "# Ensure index is reset and available as a column\n",
    "regression_results_df.reset_index(inplace=True)\n",
    "regression_results_df = regression_results_df.rename(columns={'index': 'model_name'})\n",
    "\n",
    "regression_results_df[\"model_name_custom\"] = regression_results_df[\"model_name\"].map(custom_labels)\n",
    "\n",
    "# Create the Altair chart\n",
    "points = alt.Chart(regression_results_df).mark_point(\n",
    "    filled=True,\n",
    "    color='red',\n",
    "    size=50  # Increase point size\n",
    ").encode(\n",
    "    x=alt.X('coef:Q').title('Coefficient'),\n",
    "    y=alt.Y('model_name_custom:N').title('LLM Name').sort(\n",
    "        field='coef',  # Sort by coefficient values\n",
    "        order='descending'\n",
    "    )\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=300\n",
    ")\n",
    "\n",
    "# Add error bars\n",
    "error_bars = points.mark_rule(\n",
    "    strokeWidth=2  # Increase width of error bars\n",
    ").encode(\n",
    "    x='ci_lower:Q',\n",
    "    x2='ci_upper:Q',\n",
    "    size=alt.value(2)  # Set the width of error bars\n",
    ")\n",
    "\n",
    "# Add vertical line at x = 0.71\n",
    "vertical_line = alt.Chart(pd.DataFrame({'x': [0.71]})).mark_rule(\n",
    "    color='blue',\n",
    "    strokeDash=[4, 4],  # Make it dashed\n",
    "    strokeWidth=2\n",
    ").encode(\n",
    "    x='x:Q',\n",
    "    color=alt.value('#0868ac')  # Specify the color directly\n",
    ")\n",
    "\n",
    "# Add label for vertical line\n",
    "label = alt.Chart(pd.DataFrame({'x': [0.71], 'y': [regression_results_df['model_name_custom'].iloc[0]]})).mark_text(\n",
    "    text='Human Experts',\n",
    "    align='center',\n",
    "    dx=5,  # Adjust text position\n",
    "    dy=-10,\n",
    "    fontSize=14,\n",
    "    fontWeight='bold',\n",
    ").encode(\n",
    "    x='x:Q',\n",
    "    y=alt.value(0),  # Adjust position if necessary\n",
    "    color=alt.value('#0868ac')  # Specify the color directly\n",
    ")\n",
    "\n",
    "# Define custom x-axis labels\n",
    "custom_labels_df = pd.DataFrame({\n",
    "    'x': [regression_results_df['coef'].min(), regression_results_df['coef'].max()],\n",
    "    'text': ['Less susceptible to spin', 'More susceptible to spin']\n",
    "})\n",
    "\n",
    "# Define custom x-axis labels\n",
    "left_arrow_df = pd.DataFrame({\n",
    "    'x': [0.2],\n",
    "    'text': ['←']\n",
    "})\n",
    "\n",
    "# Define custom x-axis labels\n",
    "right_arrow_df = pd.DataFrame({\n",
    "    'x': [7.7],\n",
    "    'text': ['→']\n",
    "})\n",
    "\n",
    "# Create a text layer for custom x-axis labels\n",
    "custom_x_labels = alt.Chart(custom_labels_df).mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    dy=195,  # Adjust vertical positioning\n",
    "    dx=24, # adjust horizontal positioning\n",
    "    fontSize=14,\n",
    "    fontWeight='bold',\n",
    ").encode(\n",
    "    x='x:Q',\n",
    "    text='text:N',\n",
    "    color=alt.value('#0868ac')  # Specify the color directly\n",
    ")\n",
    "\n",
    "# Create a text layer for custom x-axis labels\n",
    "custom_x_left_arrow = alt.Chart(left_arrow_df).mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    dy=195,  # Adjust vertical positioning\n",
    "    dx=10, # adjust horizontal positioning\n",
    "    fontSize=24,\n",
    "    fontWeight='bold',\n",
    ").encode(\n",
    "    x='x:Q',\n",
    "    text='text:N',\n",
    "    color=alt.value('#0868ac')  # Specify the color directly\n",
    ")\n",
    "\n",
    "# Create a text layer for custom x-axis labels\n",
    "custom_x_right_arrow = alt.Chart(right_arrow_df).mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    dy=195,  # Adjust vertical positioning\n",
    "    dx=0, # adjust horizontal positioning\n",
    "    fontSize=24,\n",
    "    fontWeight='bold',\n",
    ").encode(\n",
    "    x='x:Q',\n",
    "    text='text:N',\n",
    "    color=alt.value('#0868ac')  # Specify the color directly\n",
    ")\n",
    "\n",
    "# Combine all layers, including the new x-axis labels\n",
    "chart = alt.layer(error_bars, points, vertical_line, label, custom_x_labels, custom_x_left_arrow, custom_x_right_arrow).configure_axis(\n",
    "    labelFontSize=16,\n",
    "    titleFontSize=18\n",
    ").configure_title(\n",
    "    fontSize=20\n",
    ")\n",
    "\n",
    "# # Save to HTML\n",
    "chart.save(\"./plots/simple_regression_benefit_data.html\")\n",
    "\n",
    "chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLS LLM Interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all PLS outputs from all LLMs\n",
    "enc = tiktoken.get_encoding(\"o200k_base\") # for gpt-4o and gpt-4o mini\n",
    "\n",
    "model_token_stats = {}\n",
    "number_of_tokens_total = [] # for all models\n",
    "for model_name in model_names:\n",
    "    csv_file_path = f\"./pls_outputs/{model_name}/{model_name}_outputs.csv\"\n",
    "    data = pd.read_csv(csv_file_path)\n",
    "    # calculate the number of tokens for each row in plain_language_summary\n",
    "    plain_language_summaries = data['plain_language_summary'].tolist()\n",
    "\n",
    "    number_of_tokens = []\n",
    "    for summary in plain_language_summaries:\n",
    "        token_integers = enc.encode(summary)\n",
    "        number_of_tokens.append(len(token_integers))\n",
    "        number_of_tokens_total.append(len(token_integers))\n",
    "\n",
    "    # average number of tokens\n",
    "    average_number_of_tokens = np.mean(number_of_tokens)\n",
    "    # SD of tokens\n",
    "    sd_number_of_tokens = np.std(number_of_tokens)\n",
    "    model_token_stats[model_name] = {\"average_number_of_tokens\": average_number_of_tokens, \"sd_number_of_tokens\": sd_number_of_tokens}\n",
    "\n",
    "model_token_stats_df = pd.DataFrame(model_token_stats).T\n",
    "model_token_stats_df[\"model_name\"] = model_token_stats_df.index\n",
    "\n",
    "model_token_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average across all\n",
    "average_number_of_tokens = np.mean(number_of_tokens_total)\n",
    "sd_number_of_tokens = np.std(number_of_tokens_total)\n",
    "\n",
    "average_number_of_tokens, sd_number_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average across all models\n",
    "average_number_of_tokens = model_token_stats_df[\"average_number_of_tokens\"].mean()\n",
    "sd_number_of_tokens = model_token_stats_df[\"sd_number_of_tokens\"].mean()\n",
    "\n",
    "average_number_of_tokens, sd_number_of_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following files were manually created\n",
    "claude_evaluator_results = pd.read_json(\"./pls_outputs/_interpretation_eval_results/claude_3.5-sonnet/claude_3.5-sonnet_interpretation_overall_metrics.json\", orient=\"index\")\n",
    "gpt4o_mini_evaluator_results = pd.read_json(\"./pls_outputs/_interpretation_eval_results/gpt4o-mini/gpt4o-mini_interpretation_overall_metrics.json\", orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_gpt4o_mini_model_stats_df = calculate_model_stats(gpt4o_mini_evaluator_results, method_name = \"GPT4o Mini\")\n",
    "\n",
    "pls_gpt4o_mini_model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_claude_model_stats_df = calculate_model_stats(claude_evaluator_results, method_name = \"Claude 3.5 Sonnet\")\n",
    "\n",
    "pls_claude_model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two dataframes\n",
    "all_pls_model_stats_df = pd.concat([pls_gpt4o_mini_model_stats_df, pls_claude_model_stats_df], ignore_index=True)\n",
    "\n",
    "all_pls_model_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create altair grouped barchart\n",
    "# grouped by metric and evaluator\n",
    "\n",
    "# Create a mapping for custom facet titles\n",
    "facet_title_mapping = {\n",
    "    'benefit_answer': 'Benefit',\n",
    "    'rigor_answer': 'Rigor',\n",
    "    'importance_answer': 'Importance',\n",
    "    'full_text_answer': 'Full-Text',\n",
    "    'another_trial_answer': 'Another Trial'\n",
    "}\n",
    "\n",
    "# Define the desired order for the facets\n",
    "facet_order = ['Benefit', 'Rigor', 'Importance', 'Full-Text', 'Another Trial']\n",
    "\n",
    "color_mapping = {\n",
    "    'Claude 3.5 Sonnet': '#0868ac',  \n",
    "    'GPT4o Mini': '#43a2ca',  \n",
    "}\n",
    "\n",
    "method_order = ['Claude 3.5 Sonnet', 'GPT4o Mini']\n",
    "\n",
    "# Apply the mapping as a calculated field\n",
    "chart_data = all_pls_model_stats_df.copy()\n",
    "chart_data['metric'] = chart_data['metric'].map(facet_title_mapping)\n",
    "\n",
    "# Configure global font sizes\n",
    "chart_config = {\n",
    "    \"axis\": {\"labelFontSize\": 20, \"titleFontSize\": 22},  # Axis labels and titles\n",
    "    \"header\": {\"labelFontSize\": 20, \"titleFontSize\": 22},  # Facet headers\n",
    "    \"legend\": {\"labelFontSize\": 18, \"titleFontSize\": 20},  # Legend labels and titles\n",
    "    \"text\": {\"fontSize\": 20},  # Text mark size\n",
    "}\n",
    "\n",
    "# Bar chart\n",
    "bars = alt.Chart(chart_data).mark_bar().encode(\n",
    "    x=alt.X('evaluator:N', title=None, axis=alt.Axis(labelAngle=-45), sort = method_order),\n",
    "    y=alt.Y('mean_diff:Q', title='Mean Difference'),\n",
    "    color=alt.Color('evaluator:N', title='Evaluator', legend=None, scale=alt.Scale(domain=list(color_mapping.keys()), range=list(color_mapping.values())))\n",
    ").properties(\n",
    "    width=120,  # Set the width to 300 pixels\n",
    "    height=250  # Set the height to 300 pixels\n",
    ")\n",
    "\n",
    "# Error bars\n",
    "error_bars = alt.Chart(chart_data).mark_errorbar().encode(\n",
    "    alt.X(\"evaluator:N\", sort = method_order),\n",
    "    alt.Y(\"ci_lower:Q\").title(\"Mean Difference\"),\n",
    "    alt.Y2(\"ci_upper:Q\"),\n",
    "    strokeWidth=alt.value(2),\n",
    "    color=alt.value('gray')\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "text = bars.mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    fontWeight='bold',\n",
    "    dy=alt.expr(expr=alt.expr.if_(alt.datum.mean_diff >= 0, -1, 20))  # Adjust the position of the text    \n",
    ").encode(\n",
    "    text=alt.Text('mean_diff:Q', format='.2f'),\n",
    "    color=alt.value('black')  # Set text color to black\n",
    ")\n",
    "\n",
    "# Combine layers and facet\n",
    "chart = alt.layer(bars, text, error_bars, data=chart_data).facet(\n",
    "    column=alt.Column('metric:N', title=None, sort=facet_order),\n",
    ").configure(**chart_config)  # Apply\n",
    "\n",
    "# save to html\n",
    "chart.save(\"./plots/pls_evaluator_comparison_by_measures.html\")\n",
    "\n",
    "chart\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
