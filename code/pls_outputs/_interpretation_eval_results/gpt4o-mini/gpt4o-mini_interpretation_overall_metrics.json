{
    "gpt4o": {
        "benefit_answer_mean_diff": 3.6,
        "rigor_answer_mean_diff": 1.3999999999999995,
        "importance_answer_mean_diff": 2.7666666666666666,
        "full_text_answer_mean_diff": 3.6,
        "another_trial_answer_mean_diff": 3.833333333333333,
        "overall_avg": 3.04
    },
    "gpt4o-mini": {
        "benefit_answer_mean_diff": 3.5333333333333337,
        "rigor_answer_mean_diff": 1.3666666666666663,
        "importance_answer_mean_diff": 2.8,
        "full_text_answer_mean_diff": 3.8000000000000003,
        "another_trial_answer_mean_diff": 3.9000000000000004,
        "overall_avg": 3.08
    },
    "gpt35": {
        "benefit_answer_mean_diff": 3.6333333333333333,
        "rigor_answer_mean_diff": 1.333333333333333,
        "importance_answer_mean_diff": 2.8000000000000003,
        "full_text_answer_mean_diff": 3.766666666666667,
        "another_trial_answer_mean_diff": 3.9000000000000004,
        "overall_avg": 3.086666666666667
    },
    "gemini_1.5_flash": {
        "benefit_answer_mean_diff": 3.6333333333333333,
        "rigor_answer_mean_diff": 1.333333333333333,
        "importance_answer_mean_diff": 2.8000000000000003,
        "full_text_answer_mean_diff": 3.7666666666666666,
        "another_trial_answer_mean_diff": 3.8,
        "overall_avg": 3.0666666666666664
    },
    "gemini_1.5_flash-8B": {
        "benefit_answer_mean_diff": 3.6,
        "rigor_answer_mean_diff": 1.3666666666666663,
        "importance_answer_mean_diff": 2.666666666666667,
        "full_text_answer_mean_diff": 3.6333333333333333,
        "another_trial_answer_mean_diff": 3.833333333333333,
        "overall_avg": 3.02
    },
    "claude_3.5-sonnet": {
        "benefit_answer_mean_diff": 3.5333333333333337,
        "rigor_answer_mean_diff": 1.2999999999999998,
        "importance_answer_mean_diff": 2.833333333333333,
        "full_text_answer_mean_diff": 3.6666666666666665,
        "another_trial_answer_mean_diff": 4.0,
        "overall_avg": 3.0666666666666664
    },
    "claude_3.5-haiku": {
        "benefit_answer_mean_diff": 3.5333333333333337,
        "rigor_answer_mean_diff": 1.333333333333333,
        "importance_answer_mean_diff": 2.7666666666666666,
        "full_text_answer_mean_diff": 3.666666666666667,
        "another_trial_answer_mean_diff": 3.8,
        "overall_avg": 3.02
    },
    "alpacare-7B": {
        "benefit_answer_mean_diff": 3.6333333333333333,
        "rigor_answer_mean_diff": 1.333333333333333,
        "importance_answer_mean_diff": 2.7666666666666666,
        "full_text_answer_mean_diff": 3.8000000000000003,
        "another_trial_answer_mean_diff": 3.7666666666666666,
        "overall_avg": 3.06
    },
    "biomistral7B": {
        "benefit_answer_mean_diff": 3.566666666666667,
        "rigor_answer_mean_diff": 1.2666666666666666,
        "importance_answer_mean_diff": 2.7333333333333334,
        "full_text_answer_mean_diff": 3.733333333333333,
        "another_trial_answer_mean_diff": 3.8,
        "overall_avg": 3.02
    },
    "llama2_chat-7B": {
        "benefit_answer_mean_diff": 3.6333333333333333,
        "rigor_answer_mean_diff": 1.333333333333333,
        "importance_answer_mean_diff": 2.666666666666667,
        "full_text_answer_mean_diff": 3.533333333333333,
        "another_trial_answer_mean_diff": 3.9333333333333336,
        "overall_avg": 3.02
    },
    "llama2_chat-13B": {
        "benefit_answer_mean_diff": 3.6333333333333333,
        "rigor_answer_mean_diff": 1.3999999999999995,
        "importance_answer_mean_diff": 2.7,
        "full_text_answer_mean_diff": 3.6333333333333337,
        "another_trial_answer_mean_diff": 3.8,
        "overall_avg": 3.033333333333333
    },
    "llama2_chat-70B": {
        "benefit_answer_mean_diff": 3.6,
        "rigor_answer_mean_diff": 1.4666666666666668,
        "importance_answer_mean_diff": 2.6333333333333337,
        "full_text_answer_mean_diff": 3.733333333333334,
        "another_trial_answer_mean_diff": 3.9333333333333336,
        "overall_avg": 3.0733333333333337
    },
    "llama3_instruct-8B": {
        "benefit_answer_mean_diff": 3.566666666666667,
        "rigor_answer_mean_diff": 1.3999999999999995,
        "importance_answer_mean_diff": 2.7,
        "full_text_answer_mean_diff": 3.6333333333333333,
        "another_trial_answer_mean_diff": 3.966666666666667,
        "overall_avg": 3.0533333333333332
    },
    "llama3_instruct-70B": {
        "benefit_answer_mean_diff": 3.566666666666667,
        "rigor_answer_mean_diff": 1.3999999999999995,
        "importance_answer_mean_diff": 2.7,
        "full_text_answer_mean_diff": 3.6333333333333333,
        "another_trial_answer_mean_diff": 3.966666666666667,
        "overall_avg": 3.0533333333333332
    },
    "med42-8B": {
        "benefit_answer_mean_diff": 3.6666666666666665,
        "rigor_answer_mean_diff": 1.2666666666666666,
        "importance_answer_mean_diff": 2.7,
        "full_text_answer_mean_diff": 3.6666666666666665,
        "another_trial_answer_mean_diff": 3.866666666666667,
        "overall_avg": 3.033333333333333
    },
    "med42-70B": {
        "benefit_answer_mean_diff": 3.6666666666666665,
        "rigor_answer_mean_diff": 1.333333333333333,
        "importance_answer_mean_diff": 2.666666666666667,
        "full_text_answer_mean_diff": 3.766666666666666,
        "another_trial_answer_mean_diff": 3.8666666666666663,
        "overall_avg": 3.0599999999999996
    },
    "olmo2_instruct-7B": {
        "benefit_answer_mean_diff": 3.5000000000000004,
        "rigor_answer_mean_diff": 1.4000000000000004,
        "importance_answer_mean_diff": 2.7333333333333334,
        "full_text_answer_mean_diff": 3.766666666666667,
        "another_trial_answer_mean_diff": 3.9,
        "overall_avg": 3.06
    },
    "olmo2_instruct-13B": {
        "benefit_answer_mean_diff": 3.566666666666667,
        "rigor_answer_mean_diff": 1.4000000000000004,
        "importance_answer_mean_diff": 2.7666666666666666,
        "full_text_answer_mean_diff": 3.6333333333333333,
        "another_trial_answer_mean_diff": 3.766666666666666,
        "overall_avg": 3.026666666666667
    },
    "openbiollm-8B": {
        "benefit_answer_mean_diff": 3.6,
        "rigor_answer_mean_diff": 1.2999999999999998,
        "importance_answer_mean_diff": 2.7666666666666666,
        "full_text_answer_mean_diff": 3.566666666666667,
        "another_trial_answer_mean_diff": 3.9000000000000004,
        "overall_avg": 3.026666666666667
    },
    "openbiollm-70B": {
        "benefit_answer_mean_diff": 3.6,
        "rigor_answer_mean_diff": 1.2999999999999998,
        "importance_answer_mean_diff": 2.666666666666667,
        "full_text_answer_mean_diff": 3.6999999999999997,
        "another_trial_answer_mean_diff": 3.866666666666667,
        "overall_avg": 3.026666666666667
    },
    "mistral_instruct7B": {
        "benefit_answer_mean_diff": 3.6,
        "rigor_answer_mean_diff": 1.3666666666666663,
        "importance_answer_mean_diff": 2.7333333333333334,
        "full_text_answer_mean_diff": 3.7000000000000006,
        "another_trial_answer_mean_diff": 3.866666666666667,
        "overall_avg": 3.0533333333333337
    },
    "biomedgpt7B": {
        "benefit_answer_mean_diff": 3.566666666666667,
        "rigor_answer_mean_diff": 1.3666666666666671,
        "importance_answer_mean_diff": 2.7,
        "full_text_answer_mean_diff": 3.6999999999999997,
        "another_trial_answer_mean_diff": 3.833333333333333,
        "overall_avg": 3.033333333333333
    }
}