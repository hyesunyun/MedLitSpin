{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_json_file' from 'utils' (/Users/hyesunyun/Documents/nlp/llms_spin_med_lit/MedLitSpin/code/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_csv_file, save_dataset_to_json, load_json_file\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_json_file' from 'utils' (/Users/hyesunyun/Documents/nlp/llms_spin_med_lit/MedLitSpin/code/utils.py)"
     ]
    }
   ],
   "source": [
    "from utils import load_csv_file, save_dataset_to_json\n",
    "import random\n",
    "import tiktoken\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full list of models\n",
    "models = [\n",
    "    \"alpacare-7B\",\n",
    "    \"biomedgpt7B\",\n",
    "    \"biomistral7B\",\n",
    "    \"claude_3.5-haiku\", \n",
    "    \"claude_3.5-sonnet\",\n",
    "    \"gemini_1.5_flash\",\n",
    "    \"gemini_1.5_flash-8B\",\n",
    "    \"gpt4o\", \n",
    "    \"gpt4o-mini\",\n",
    "    \"gpt35\",\n",
    "    \"llama2_chat-7B\",\n",
    "    \"llama2_chat-13B\",\n",
    "    \"llama2_chat-70B\",\n",
    "    \"llama3_instruct-8B\",\n",
    "    \"llama3_instruct-70B\",\n",
    "    \"med42-8B\",\n",
    "    \"med42-70B\",\n",
    "    \"mistral_instruct7B\",\n",
    "    \"olmo2_instruct-7B\",\n",
    "    \"olmo2_instruct-13B\",\n",
    "    \"openbiollm-8B\",\n",
    "    \"openbiollm-70B\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_summaries = []\n",
    "for model in models:\n",
    "    file_path = f\"./pls_outputs/{model}/{model}_outputs.csv\"\n",
    "    summaries = load_csv_file(file_path)\n",
    "\n",
    "    # get unique PMIDs\n",
    "    pmids = list(set([summary['PMID'] for summary in summaries]))\n",
    "    # shuffle PMIDs\n",
    "    random.shuffle(pmids)\n",
    "    # sample 5 PMIDs randomly with a seed\n",
    "    sampled_pmids = random.sample(pmids, 5)\n",
    "\n",
    "    # get summaries for sampled PMIDs\n",
    "    for summary in summaries:\n",
    "        summary_pmid = summary[\"PMID\"]\n",
    "        if summary_pmid in sampled_pmids:\n",
    "            plain_language_summary = summary[\"plain_language_summary\"]\n",
    "            # parts = plain_language_summary.split(\"\\n\\n\", 1)  # Split at the first occurrence only\n",
    "            # plain_language_summary = parts[1] if len(parts) > 1 else plain_language_summary\n",
    "            sampled_summary = {\n",
    "                \"model\": model,\n",
    "                \"pmid\": summary_pmid,\n",
    "                \"from_abstract_type\": summary[\"abstract_type\"],\n",
    "                \"summary\": plain_language_summary\n",
    "            }\n",
    "            sampled_summaries.append(sampled_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 911)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./pls_outputs/sampled_summaries_for_human_eval.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m     sampled_summaries \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./pls_outputs/sampled_summaries_for_human_eval.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m sampled_summaries:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MedLitSpin/lib/python3.11/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MedLitSpin/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MedLitSpin/lib/python3.11/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 911)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./pls_outputs/sampled_summaries_for_human_eval.json', 'w') as outfile:\n",
    "    for entry in sampled_summaries:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read jsonl file\n",
    "sampled_summaries = []\n",
    "with open('./pls_outputs/sampled_summaries_for_human_eval.json', 'r') as f:\n",
    "    for line in f:\n",
    "        sampled_summaries.append(json.loads(line))\n",
    "\n",
    "# add a unique id for each entry\n",
    "for i, entry in enumerate(sampled_summaries):\n",
    "    # change the summary key to text\n",
    "    entry[\"text\"] = entry.pop(\"summary\")\n",
    "\n",
    "# save the dataset to jsonl\n",
    "with open('./pls_outputs/sampled_summaries_for_human_eval.json', 'w') as outfile:\n",
    "    for entry in sampled_summaries:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document saved as ./pls_outputs/sampled_plain_language_summaries.docx\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import json\n",
    "\n",
    "# read from json file\n",
    "with open(\"./pls_outputs/sampled_summaries_for_human_eval.json\", \"r\") as f:\n",
    "    sampled_summaries = json.load(f)\n",
    "data = sampled_summaries\n",
    "\n",
    "def create_word_doc(data, filename=\"output.docx\"):\n",
    "    doc = Document()\n",
    "    doc.add_heading('Sampled Summaries for Evaluation', 0)\n",
    "    doc.add_paragraph(f\"Number of sampled summaries: {len(data)}\")\n",
    "    doc.add_paragraph(f\"Number of unique PMIDs: {len(set([summary['pmid'] for summary in data]))}\")\n",
    "    doc.add_paragraph(\"\\n\")\n",
    "\n",
    "    for entry in data:\n",
    "        pmid = entry[\"pmid\"]\n",
    "        model = entry[\"model\"]\n",
    "        summary = entry[\"text\"]\n",
    "        doc.add_paragraph(f\"PMID: {pmid}\", style='Heading2')\n",
    "        \n",
    "        llm_name_paragraph = doc.add_paragraph()\n",
    "        llm_name_paragraph.add_run(\"LLM Name:\").bold = True\n",
    "        llm_name_paragraph.add_run(f\" {model}\")\n",
    "        \n",
    "        summary_paragraph = doc.add_paragraph()\n",
    "        summary_paragraph.add_run(\"Plain Language Summary:\").bold = True\n",
    "        summary_paragraph.add_run(f\" {summary}\")\n",
    "        \n",
    "        doc.add_paragraph(\"\\n\")\n",
    "\n",
    "    # Save the document\n",
    "    doc.save(filename)\n",
    "    print(f\"Document saved as {filename}\")\n",
    "\n",
    "create_word_doc(sampled_summaries, \"./pls_outputs/sampled_plain_language_summaries.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sampled summaries: 220\n",
      "Number of unique PMIDs: 30\n",
      "Average token count: 204.9318181818182\n",
      "PMID count: {'17264336': 16, '20448107': 12, '16314619': 12, '18955454': 12, '18794551': 12, '9093724': 10, '21041710': 10, '10547391': 10, '20673585': 10, '21060024': 10, '11261827': 10, '17179098': 10, '19273714': 8, '20973267': 8, '16504757': 6, '20800381': 6, '20564068': 6, '21471562': 6, '15947110': 6, '22112969': 6, '12177098': 4, '20530276': 4, '20153039': 4, '17530429': 4, '21399726': 4, '10637238': 4, '20087643': 4, '17134892': 2, '17173959': 2, '16148021': 2}\n",
      "Abstract type count: {'no_spin': 110, 'spin': 110}\n"
     ]
    }
   ],
   "source": [
    "# stats of the sampled summaries\n",
    "print(f\"Number of sampled summaries: {len(sampled_summaries)}\")\n",
    "print(f\"Number of unique PMIDs: {len(set([summary['pmid'] for summary in sampled_summaries]))}\")\n",
    "\n",
    "# get average token of summaries\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "token_counts = [len(encoding.encode(summary[\"summary\"])) for summary in sampled_summaries]\n",
    "average_token_count = sum(token_counts) / len(token_counts)\n",
    "print(f\"Average token count: {average_token_count}\")\n",
    "\n",
    "# count for each unique PMID\n",
    "pmids = [summary[\"pmid\"] for summary in sampled_summaries]\n",
    "pmid_count = {pmid: pmids.count(pmid) for pmid in set(pmids)}\n",
    "# sort by count\n",
    "pmid_count = dict(sorted(pmid_count.items(), key=lambda item: item[1], reverse=True))\n",
    "print(f\"PMID count: {pmid_count}\")\n",
    "\n",
    "# count for each from_abstract_type\n",
    "abstract_types = [summary[\"from_abstract_type\"] for summary in sampled_summaries]\n",
    "abstract_type_count = {abstract_type: abstract_types.count(abstract_type) for abstract_type in set(abstract_types)}\n",
    "print(f\"Abstract type count: {abstract_type_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedLitSpin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
