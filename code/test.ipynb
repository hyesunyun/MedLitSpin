{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average differences for 'benefit_answer':\n",
      "-6.0\n",
      "Average differences for 'rigor_answer':\n",
      "-2.0\n",
      "Average differences for 'importance_answer':\n",
      "0.0\n",
      "Average differences for 'full_text_answer':\n",
      "-3.0\n",
      "Average differences for 'another_trial_answer':\n",
      "-5.0\n",
      "\n",
      "Overall average difference across all answers:\n",
      "-3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('./outputs/gpt35/gpt35_outputs.csv')\n",
    "\n",
    "# get unique PMID values in a list\n",
    "pmids = df['PMID'].unique()\n",
    "\n",
    "column_names = [\"benefit_answer\", \"rigor_answer\", \"importance_answer\", \"full_text_answer\", \"another_trial_answer\"]\n",
    "\n",
    "metrics = {}\n",
    "for col in column_names:\n",
    "    column_dffs = []\n",
    "    for pmid in pmids:\n",
    "        # Get the rows for the current PMID\n",
    "        pmid_rows = df[df['PMID'] == pmid]\n",
    "        # get the 'spin' answer\n",
    "        spin_answer = pmid_rows.loc[pmid_rows['abstract_type'] == 'spin', col].values[0]\n",
    "        # get the 'no spin' answer\n",
    "        no_spin_answer = pmid_rows.loc[pmid_rows['abstract_type'] == 'no_spin', col].values[0]\n",
    "        # subtract the 'spin' answer from the 'no spin' answer\n",
    "        diff = no_spin_answer - spin_answer\n",
    "        \n",
    "        column_dffs.append(diff)\n",
    "\n",
    "    # Average all the differences for each column\n",
    "    column_avg = diff.mean()\n",
    "\n",
    "    metrics[f\"{col}_avg\"] = column_avg\n",
    "    print(f\"Average differences for '{col}':\")\n",
    "    print(column_avg)\n",
    "\n",
    "# Average across all columns\n",
    "overall_avg = sum(metrics.values()) / len(metrics)\n",
    "metrics['overall_avg'] = overall_avg\n",
    "\n",
    "print(f\"\\nOverall average difference across all answers:\")\n",
    "print(overall_avg)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('./outputs/gpt35/gpt35_differences_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedLitSpin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
