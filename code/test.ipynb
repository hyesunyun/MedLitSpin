{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('./outputs/gpt35/gpt35_outputs.csv')\n",
    "\n",
    "# get unique PMID values in a list\n",
    "pmids = df['PMID'].unique()\n",
    "\n",
    "column_names = [\"benefit_answer\", \"rigor_answer\", \"importance_answer\", \"full_text_answer\", \"another_trial_answer\"]\n",
    "\n",
    "metrics = {}\n",
    "for col in column_names:\n",
    "    column_dffs = []\n",
    "    for pmid in pmids:\n",
    "        # Get the rows for the current PMID\n",
    "        pmid_rows = df[df['PMID'] == pmid]\n",
    "        # get the 'spin' answer\n",
    "        spin_answer = pmid_rows.loc[pmid_rows['abstract_type'] == 'spin', col].values[0]\n",
    "        # get the 'no spin' answer\n",
    "        no_spin_answer = pmid_rows.loc[pmid_rows['abstract_type'] == 'no_spin', col].values[0]\n",
    "        # subtract the 'spin' answer from the 'no spin' answer\n",
    "        diff = no_spin_answer - spin_answer\n",
    "        \n",
    "        column_dffs.append(diff)\n",
    "\n",
    "    # Average all the differences for each column\n",
    "    column_avg = diff.mean()\n",
    "\n",
    "    metrics[f\"{col}_avg\"] = column_avg\n",
    "    print(f\"Average differences for '{col}':\")\n",
    "    print(column_avg)\n",
    "\n",
    "# Average across all columns\n",
    "overall_avg = sum(metrics.values()) / len(metrics)\n",
    "metrics['overall_avg'] = overall_avg\n",
    "\n",
    "print(f\"\\nOverall average difference across all answers:\")\n",
    "print(overall_avg)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('./outputs/gpt35/gpt35_differences_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference for 'benefit_answer': 4.4\n",
      "Mean difference for 'rigor_answer': 0.0\n",
      "Mean difference for 'importance_answer': 0.6333333333333337\n",
      "Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...\n",
      "PMIDs with 'Error' or empty string values in column 'full_text_answer': [20087643, 11261827, 20530276, 15947110, 22112969, 12177098, 21471562, 16148021, 18794551, 17179098, 16314619, 9093724]\n",
      "Number of rows after removing 'Error' or empty string values: 36\n",
      "Mean difference for 'full_text_answer': 1.0555555555555554\n",
      "Mean difference for 'another_trial_answer': 4.933333333333334\n",
      "\n",
      "Overall mean difference across all answers: 2.2044444444444444\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('./eval_outputs/openbiollm-70B/openbiollm-70B_interpretation_outputs.csv')\n",
    "\n",
    "# column names for the 5 questions\n",
    "column_names = [\"benefit_answer\", \"rigor_answer\", \"importance_answer\", \"full_text_answer\", \"another_trial_answer\"]\n",
    "\n",
    "diff_metrics = {}\n",
    "for col in column_names:\n",
    "    df_copy = df.copy()\n",
    "    #change to string the column values\n",
    "    df_copy[col] = df_copy[col].astype(str)\n",
    "    # check if column values have any Error or empty string values for model outputs\n",
    "    if df_copy[col].apply(lambda x: \"Error\" in x or x == \"\" or x == \"nan\").any():\n",
    "        print(f\"Column '{col}' has some 'Error' or empty string values. Removing these rows from the metrics...\")\n",
    "        # remove pmids rows with 'Error' or empty string values\n",
    "        error_pmids = df_copy[df_copy[col].apply(lambda x: \"Error\" in x or x == \"\" or x == \"nan\")]['PMID'].tolist()\n",
    "        # unique ids\n",
    "        error_pmids = list(set(error_pmids))\n",
    "        df_copy = df_copy[~df_copy['PMID'].isin(error_pmids)]\n",
    "        print(f\"PMIDs with 'Error' or empty string values in column '{col}': {error_pmids}\")\n",
    "        print(f\"Number of rows after removing 'Error' or empty string values: {len(df_copy)}\")\n",
    "\n",
    "    # for each column, get the average of spin and no_spin answers\n",
    "    spin_avg = df_copy[df_copy['abstract_type'] == 'spin'][col].astype(float).mean()\n",
    "    no_spin_avg = df_copy[df_copy['abstract_type'] == 'no_spin'][col].astype(float).mean()\n",
    "    \n",
    "    # print(f\"Average for '{col}' (spin): {spin_avg}\")\n",
    "    # print(f\"Average for '{col}' (no_spin): {no_spin_avg}\")\n",
    "\n",
    "    diff = spin_avg - no_spin_avg\n",
    "    diff_metrics[f\"{col}_diff\"] = diff\n",
    "    print(f\"Mean difference for '{col}': {diff}\")\n",
    "\n",
    "# Average across all columns\n",
    "overall_avg = mean(diff_metrics.values())\n",
    "diff_metrics['overall_avg'] = overall_avg\n",
    "\n",
    "print(f\"\\nOverall mean difference across all answers: {overall_avg}\")\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('./eval_outputs/openbiollm-70B/openbiollm-70B_mean_differences_metrics.json', 'w') as f:\n",
    "    json.dump(diff_metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file: ./eval_outputs/biomistral7B/biomistral7B_interpretation_outputs.csv\n",
      "Column 'benefit_answer' ======= Number of PMIDs with error: 6\n",
      "Column 'rigor_answer' ======= Number of PMIDs with error: 18\n",
      "Column 'importance_answer' ======= Number of PMIDs with error: 10\n",
      "Column 'full_text_answer' ======= Number of PMIDs with error: 4\n",
      "Column 'another_trial_answer' ======= Number of PMIDs with error: 2\n",
      "#################\n",
      "\n",
      "Reading file: ./eval_outputs/openbiollm-8B/openbiollm-8B_interpretation_outputs.csv\n",
      "Column 'benefit_answer' ======= Number of PMIDs with error: 11\n",
      "Column 'rigor_answer' ======= Number of PMIDs with error: 2\n",
      "Column 'importance_answer' ======= Number of PMIDs with error: 3\n",
      "Column 'full_text_answer' ======= Number of PMIDs with error: 5\n",
      "Column 'another_trial_answer' ======= Number of PMIDs with error: 22\n",
      "#################\n",
      "\n",
      "Reading file: ./eval_outputs/openbiollm-70B/openbiollm-70B_interpretation_outputs.csv\n",
      "Column 'benefit_answer' ======= Number of PMIDs with error: 0\n",
      "Column 'rigor_answer' ======= Number of PMIDs with error: 0\n",
      "Column 'importance_answer' ======= Number of PMIDs with error: 0\n",
      "Column 'full_text_answer' ======= Number of PMIDs with error: 12\n",
      "Column 'another_trial_answer' ======= Number of PMIDs with error: 0\n",
      "#################\n",
      "\n",
      "Reading file: ./eval_outputs/llama2_chat-70B/llama2_chat-70B_interpretation_outputs.csv\n",
      "Column 'benefit_answer' ======= Number of PMIDs with error: 1\n",
      "Column 'rigor_answer' ======= Number of PMIDs with error: 0\n",
      "Column 'importance_answer' ======= Number of PMIDs with error: 0\n",
      "Column 'full_text_answer' ======= Number of PMIDs with error: 0\n",
      "Column 'another_trial_answer' ======= Number of PMIDs with error: 0\n",
      "#################\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [\n",
    "    './eval_outputs/biomistral7B/biomistral7B_interpretation_outputs.csv',\n",
    "    './eval_outputs/openbiollm-8B/openbiollm-8B_interpretation_outputs.csv',\n",
    "    './eval_outputs/openbiollm-70B/openbiollm-70B_interpretation_outputs.csv',\n",
    "    './eval_outputs/llama2_chat-70B/llama2_chat-70B_interpretation_outputs.csv'\n",
    "]\n",
    "\n",
    "# Read the CSV file\n",
    "for file_path in files:\n",
    "    print(f\"\\nReading file: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # column names for the 5 questions\n",
    "    column_names = [\"benefit_answer\", \"rigor_answer\", \"importance_answer\", \"full_text_answer\", \"another_trial_answer\"]\n",
    "\n",
    "    diff_metrics = {}\n",
    "    for col in column_names:\n",
    "        #change to string the column values\n",
    "        df[col] = df[col].astype(str)\n",
    "        # check which PMIDs have errors or empty string or nan values for model outputs\n",
    "        error_pmids = df[df[col].apply(lambda x: \"Error\" in x or x == \"\" or x == \"nan\")]['PMID'].tolist()\n",
    "        # unique ids\n",
    "        error_pmids = list(set(error_pmids))\n",
    "        print(f\"Column '{col}' ======= Number of PMIDs with error: {len(error_pmids)}\")\n",
    "\n",
    "    print(\"#################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "F1: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('./eval_outputs/openbiollm-8B/openbiollm-8B_detection_outputs.csv')\n",
    "\n",
    "# check if any of results are errors\n",
    "if df[\"model_answer\"].str.contains(\"Error\").any():\n",
    "    print(\"Some of the model outputs are errors. Cannot calculate the metrics.\")\n",
    "\n",
    "# check if column values have any Error or empty string values for model outputs\n",
    "if df[\"model_answer\"].apply(lambda x: \"Error\" in x or x == \"\").any():\n",
    "    print(\"Model's output has some 'Error' or empty string values. Removing these rows from the metrics...\")\n",
    "    # remove rows with 'Error' or empty string values\n",
    "    df = df[df[\"model_answer\"].apply(lambda x: \"Error\" not in x and x != \"\")]\n",
    "    print(f\"Number of rows after removing 'Error' or empty string values: {len(df)}\")\n",
    "\n",
    "# calculate the metrics\n",
    "metrics = {}\n",
    "# convert the spin and model_answer to binary values\n",
    "df[\"ground_truth\"] = df[\"abstract_type\"].apply(lambda x: 1 if x == \"spin\" else 0)\n",
    "df[\"model_answer\"] = df[\"model_answer\"].apply(lambda x: 1 if x == \"yes\" else 0)\n",
    "\n",
    "# calculate the metrics (accuracy, precision, recall, f1 score)\n",
    "accuracy = accuracy_score(df[\"ground_truth\"], df[\"model_answer\"])\n",
    "precision = precision_score(df[\"ground_truth\"], df[\"model_answer\"])\n",
    "recall = recall_score(df[\"ground_truth\"], df[\"model_answer\"])\n",
    "f1 = f1_score(df[\"ground_truth\"], df[\"model_answer\"])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n",
    "\n",
    "metrics[\"accuracy\"] = accuracy\n",
    "metrics[\"precision\"] = precision\n",
    "metrics[\"recall\"] = recall\n",
    "metrics[\"f1\"] = f1\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('./eval_outputs/openbiollm-8B/openbiollm-8B_detection_outputs.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedLitSpin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
