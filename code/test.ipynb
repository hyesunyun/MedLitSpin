{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average differences for 'benefit_answer':\n",
      "-6.0\n",
      "Average differences for 'rigor_answer':\n",
      "-2.0\n",
      "Average differences for 'importance_answer':\n",
      "0.0\n",
      "Average differences for 'full_text_answer':\n",
      "-3.0\n",
      "Average differences for 'another_trial_answer':\n",
      "-5.0\n",
      "\n",
      "Overall average difference across all answers:\n",
      "-3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('./outputs/gpt35/gpt35_outputs.csv')\n",
    "\n",
    "# get unique PMID values in a list\n",
    "pmids = df['PMID'].unique()\n",
    "\n",
    "column_names = [\"benefit_answer\", \"rigor_answer\", \"importance_answer\", \"full_text_answer\", \"another_trial_answer\"]\n",
    "\n",
    "metrics = {}\n",
    "for col in column_names:\n",
    "    column_dffs = []\n",
    "    for pmid in pmids:\n",
    "        # Get the rows for the current PMID\n",
    "        pmid_rows = df[df['PMID'] == pmid]\n",
    "        # get the 'spin' answer\n",
    "        spin_answer = pmid_rows.loc[pmid_rows['abstract_type'] == 'spin', col].values[0]\n",
    "        # get the 'no spin' answer\n",
    "        no_spin_answer = pmid_rows.loc[pmid_rows['abstract_type'] == 'no_spin', col].values[0]\n",
    "        # subtract the 'spin' answer from the 'no spin' answer\n",
    "        diff = no_spin_answer - spin_answer\n",
    "        \n",
    "        column_dffs.append(diff)\n",
    "\n",
    "    # Average all the differences for each column\n",
    "    column_avg = diff.mean()\n",
    "\n",
    "    metrics[f\"{col}_avg\"] = column_avg\n",
    "    print(f\"Average differences for '{col}':\")\n",
    "    print(column_avg)\n",
    "\n",
    "# Average across all columns\n",
    "overall_avg = sum(metrics.values()) / len(metrics)\n",
    "metrics['overall_avg'] = overall_avg\n",
    "\n",
    "print(f\"\\nOverall average difference across all answers:\")\n",
    "print(overall_avg)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('./outputs/gpt35/gpt35_differences_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average for 'benefit_answer' (spin): 5.066666666666666\n",
      "Average for 'benefit_answer' (no_spin): 2.566666666666667\n",
      "Difference for 'benefit_answer':\n",
      "2.4999999999999996\n",
      "Average for 'rigor_answer' (spin): 7.733333333333333\n",
      "Average for 'rigor_answer' (no_spin): 7.9\n",
      "Difference for 'rigor_answer':\n",
      "-0.16666666666666696\n",
      "Average for 'importance_answer' (spin): 6.866666666666666\n",
      "Average for 'importance_answer' (no_spin): 7.5\n",
      "Difference for 'importance_answer':\n",
      "-0.6333333333333337\n",
      "Average for 'full_text_answer' (spin): 6.466666666666667\n",
      "Average for 'full_text_answer' (no_spin): 3.2333333333333334\n",
      "Difference for 'full_text_answer':\n",
      "3.2333333333333334\n",
      "Average for 'another_trial_answer' (spin): 5.833333333333333\n",
      "Average for 'another_trial_answer' (no_spin): 2.966666666666667\n",
      "Difference for 'another_trial_answer':\n",
      "2.8666666666666663\n",
      "\n",
      "Overall average across all answers:\n",
      "1.5599999999999996\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('./eval_outputs/claude_3.5-haiku/claude_3.5-haiku_interpretation_outputs.csv')\n",
    "\n",
    "column_names = [\"benefit_answer\", \"rigor_answer\", \"importance_answer\", \"full_text_answer\", \"another_trial_answer\"]\n",
    "\n",
    "metrics = {}\n",
    "for col in column_names:\n",
    "    # for each column, get the average of spin and no_spin answers\n",
    "    spin_avg = df[df['abstract_type'] == 'spin'][col].mean()\n",
    "    no_spin_avg = df[df['abstract_type'] == 'no_spin'][col].mean()\n",
    "    \n",
    "    print(f\"Average for '{col}' (spin): {spin_avg}\")\n",
    "    print(f\"Average for '{col}' (no_spin): {no_spin_avg}\")\n",
    "\n",
    "    diff = spin_avg - no_spin_avg\n",
    "    metrics[f\"{col}_diff\"] = diff\n",
    "    print(f\"Difference for '{col}':\")\n",
    "    print(diff)\n",
    "\n",
    "# Average across all columns\n",
    "overall_avg = sum(metrics.values()) / len(metrics)\n",
    "metrics['overall_avg'] = overall_avg\n",
    "\n",
    "print(f\"\\nOverall average across all answers:\")\n",
    "print(overall_avg)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('./eval_outputs/claude_3.5-haiku/claude_3.5-haiku_mean_differences_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedLitSpin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
