
     active environment : MedLitSpin
    active env location : /home/yun.hy/.conda/envs/MedLitSpin
            shell level : 2
       user config file : /home/yun.hy/.condarc
 populated config files : 
          conda version : 24.5.0
    conda-build version : 24.5.1
         python version : 3.12.4.final.0
                 solver : libmamba (default)
       virtual packages : __archspec=1=sapphirerapids
                          __conda=24.5.0=0
                          __cuda=12.3=0
                          __glibc=2.34=0
                          __linux=5.14.0=0
                          __unix=0=0
       base environment : /shared/EL9/explorer/anaconda3/2024.06  (read only)
      conda av data dir : /shared/EL9/explorer/anaconda3/2024.06/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /shared/EL9/explorer/anaconda3/2024.06/pkgs
                          /home/yun.hy/.conda/pkgs
       envs directories : /home/yun.hy/.conda/envs
                          /shared/EL9/explorer/anaconda3/2024.06/envs
               platform : linux-64
             user-agent : conda/24.5.0 requests/2.32.2 CPython/3.12.4 Linux/5.14.0-362.13.1.el9_3.x86_64 rocky/9.3 glibc/2.34 solver/libmamba conda-libmamba-solver/24.1.0 libmambapy/1.5.8 aau/0.4.4 c/. s/. e/.
                UID:GID : 1825635949:100
             netrc file : /home/yun.hy/.netrc
           offline mode : False


Running evaluation for interpreting trial results with ground truth spin/no spin labels...
Arguments Provided for the Evaluator:
Model:        gpt35
Label Mode:   gold_label
Output Path:  code/eval_outputs/gpt35
Is Debug:     None

Loading the dataset...
Loading the model...
Saving outputs from model - gpt35 to csv and json
Model outputs saved to code/eval_outputs/gpt35/gpt35_gold_labelled_interpretation_outputs.json and code/eval_outputs/gpt35/gpt35_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.933333333333333
Mean difference for 'rigor_answer': -1.8666666666666663
Mean difference for 'importance_answer': 0.833333333333333
Mean difference for 'full_text_answer': 0.7999999999999998
Mean difference for 'another_trial_answer': 1.4000000000000004

Overall mean difference across all answers: 0.8200000000000001
Arguments Provided for the Evaluator:
Model:        gpt4o
Label Mode:   gold_label
Output Path:  code/eval_outputs/gpt4o
Is Debug:     None

Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o to csv and json
Model outputs saved to code/eval_outputs/gpt4o/gpt4o_gold_labelled_interpretation_outputs.json and code/eval_outputs/gpt4o/gpt4o_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.2666666666666666
Mean difference for 'rigor_answer': -1.2999999999999998
Mean difference for 'importance_answer': -0.7666666666666666
Mean difference for 'full_text_answer': 0.7666666666666666
Mean difference for 'another_trial_answer': 1.9000000000000004

Overall mean difference across all answers: 0.3733333333333334
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Label Mode:   gold_label
Output Path:  code/eval_outputs/gpt4o-mini
Is Debug:     None

Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/eval_outputs/gpt4o-mini/gpt4o-mini_gold_labelled_interpretation_outputs.json and code/eval_outputs/gpt4o-mini/gpt4o-mini_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.7999999999999998
Mean difference for 'rigor_answer': -3.0666666666666673
Mean difference for 'importance_answer': -0.6333333333333333
Mean difference for 'full_text_answer': 0.666666666666667
Mean difference for 'another_trial_answer': 1.1333333333333333

Overall mean difference across all answers: -0.020000000000000108
Arguments Provided for the Evaluator:
Model:        gemini_1.5_flash
Label Mode:   gold_label
Output Path:  code/eval_outputs/gemini_1.5_flash
Is Debug:     None

Loading the dataset...
Loading the model...
Saving outputs from model - gemini_1.5_flash to csv and json
Model outputs saved to code/eval_outputs/gemini_1.5_flash/gemini_1.5_flash_gold_labelled_interpretation_outputs.json and code/eval_outputs/gemini_1.5_flash/gemini_1.5_flash_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.0666666666666669
Mean difference for 'rigor_answer': -1.5999999999999996
Mean difference for 'importance_answer': -0.1333333333333333
Mean difference for 'full_text_answer': -0.03333333333333366
Mean difference for 'another_trial_answer': 0.7000000000000002

Overall mean difference across all answers: 8.881784197001253e-17
Arguments Provided for the Evaluator:
Model:        gemini_1.5_flash-8B
Label Mode:   gold_label
Output Path:  code/eval_outputs/gemini_1.5_flash-8B
Is Debug:     None

Loading the dataset...
Loading the model...
Saving outputs from model - gemini_1.5_flash-8B to csv and json
Model outputs saved to code/eval_outputs/gemini_1.5_flash-8B/gemini_1.5_flash-8B_gold_labelled_interpretation_outputs.json and code/eval_outputs/gemini_1.5_flash-8B/gemini_1.5_flash-8B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 0.6999999999999997
Mean difference for 'rigor_answer': -4.266666666666667
Mean difference for 'importance_answer': -3.1333333333333333
Mean difference for 'full_text_answer': -2.4333333333333336
Mean difference for 'another_trial_answer': -0.733333333333333

Overall mean difference across all answers: -1.9733333333333334
Arguments Provided for the Evaluator:
Model:        claude_3.5-sonnet
Label Mode:   gold_label
Output Path:  code/eval_outputs/claude_3.5-sonnet
Is Debug:     None

Loading the dataset...
Loading the model...
Saving outputs from model - claude_3.5-sonnet to csv and json
Model outputs saved to code/eval_outputs/claude_3.5-sonnet/claude_3.5-sonnet_gold_labelled_interpretation_outputs.json and code/eval_outputs/claude_3.5-sonnet/claude_3.5-sonnet_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.2666666666666666
Mean difference for 'rigor_answer': -1.5000000000000009
Mean difference for 'importance_answer': -3.533333333333334
Mean difference for 'full_text_answer': 2.1666666666666665
Mean difference for 'another_trial_answer': 1.2666666666666662

Overall mean difference across all answers: -0.06666666666666714
Arguments Provided for the Evaluator:
Model:        claude_3.5-haiku
Label Mode:   gold_label
Output Path:  code/eval_outputs/claude_3.5-haiku
Is Debug:     None

Loading the dataset...
Loading the model...
Saving outputs from model - claude_3.5-haiku to csv and json
Model outputs saved to code/eval_outputs/claude_3.5-haiku/claude_3.5-haiku_gold_labelled_interpretation_outputs.json and code/eval_outputs/claude_3.5-haiku/claude_3.5-haiku_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.4666666666666663
Mean difference for 'rigor_answer': -1.2666666666666666
Mean difference for 'importance_answer': -0.5999999999999996
Mean difference for 'full_text_answer': -0.09999999999999964
Mean difference for 'another_trial_answer': 1.7999999999999998

Overall mean difference across all answers: 0.4600000000000001
Arguments Provided for the Evaluator:
Model:        olmo2_instruct-7B
Label Mode:   gold_label
Output Path:  code/eval_outputs/olmo2_instruct-7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - olmo2_instruct-7B to csv and json
Model outputs saved to code/eval_outputs/olmo2_instruct-7B/olmo2_instruct-7B_gold_labelled_interpretation_outputs.json and code/eval_outputs/olmo2_instruct-7B/olmo2_instruct-7B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['17134892', '21041710', '20153039', '18794551', '16314619', '20087643', '20530276', '15947110', '20800381']
Number of rows after removing 'Error' or empty string values: 42
Mean difference for 'benefit_answer': 3.095238095238095
Column 'rigor_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'rigor_answer': ['21041710', '17530429', '18955454', '15947110', '20800381']
Number of rows after removing 'Error' or empty string values: 50
Mean difference for 'rigor_answer': -1.5999999999999996
Mean difference for 'importance_answer': 1.0666666666666664
Mean difference for 'full_text_answer': 0.833333333333333
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['18794551', '20800381']
Number of rows after removing 'Error' or empty string values: 56
Mean difference for 'another_trial_answer': 2.0

Overall mean difference across all answers: 1.079047619047619
Arguments Provided for the Evaluator:
Model:        olmo2_instruct-13B
Label Mode:   gold_label
Output Path:  code/eval_outputs/olmo2_instruct-13B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 2, 'model.layers.29': 2, 'model.layers.30': 2, 'model.layers.31': 2, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 3, 'model.layers.39': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - olmo2_instruct-13B to csv and json
Model outputs saved to code/eval_outputs/olmo2_instruct-13B/olmo2_instruct-13B_gold_labelled_interpretation_outputs.json and code/eval_outputs/olmo2_instruct-13B/olmo2_instruct-13B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 3.466666666666667
Mean difference for 'rigor_answer': -2.3000000000000007
Mean difference for 'importance_answer': 0.06666666666666643
Mean difference for 'full_text_answer': 0.33333333333333304
Mean difference for 'another_trial_answer': 0.13333333333333375

Overall mean difference across all answers: 0.33999999999999986
Arguments Provided for the Evaluator:
Model:        mistral_instruct7B
Label Mode:   gold_label
Output Path:  code/eval_outputs/mistral_instruct7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - mistral_instruct7B to csv and json
Model outputs saved to code/eval_outputs/mistral_instruct7B/mistral_instruct7B_gold_labelled_interpretation_outputs.json and code/eval_outputs/mistral_instruct7B/mistral_instruct7B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.5999999999999996
Mean difference for 'rigor_answer': -0.36666666666666625
Mean difference for 'importance_answer': 1.0999999999999996
Mean difference for 'full_text_answer': 2.7666666666666666
Mean difference for 'another_trial_answer': 2.2

Overall mean difference across all answers: 1.66
Arguments Provided for the Evaluator:
Model:        med42-8B
Label Mode:   gold_label
Output Path:  code/eval_outputs/med42-8B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - med42-8B to csv and json
Model outputs saved to code/eval_outputs/med42-8B/med42-8B_gold_labelled_interpretation_outputs.json and code/eval_outputs/med42-8B/med42-8B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.4666666666666663
Mean difference for 'rigor_answer': -0.7333333333333334
Mean difference for 'importance_answer': -0.2333333333333334
Mean difference for 'full_text_answer': -0.7000000000000002
Mean difference for 'another_trial_answer': 1.7000000000000006

Overall mean difference across all answers: 0.3
Arguments Provided for the Evaluator:
Model:        biomistral7B
Label Mode:   gold_label
Output Path:  code/eval_outputs/biomistral7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - biomistral7B to csv and json
Model outputs saved to code/eval_outputs/biomistral7B/biomistral7B_gold_labelled_interpretation_outputs.json and code/eval_outputs/biomistral7B/biomistral7B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 0.2333333333333334
Mean difference for 'rigor_answer': -0.8833333333333337
Column 'importance_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'importance_answer': ['18794551', '10547391', '17179098', '21041710', '20530276', '21399726', '15947110', '17134892', '21060024', '16148021', '22112969', '11261827', '20800381', '17173959', '10637238', '16504757', '20564068', '18955454', '9093724', '16314619', '19273714', '12177098', '20673585', '20973267']
Number of rows after removing 'Error' or empty string values: 12
Mean difference for 'importance_answer': -0.9166666666666661
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['18794551', '10547391', '17179098', '20973267', '12177098', '20530276', '16504757', '20564068', '22112969', '20800381', '20087643', '18955454', '21060024']
Number of rows after removing 'Error' or empty string values: 34
Mean difference for 'full_text_answer': -1.2647058823529411
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['20973267']
Number of rows after removing 'Error' or empty string values: 58
Mean difference for 'another_trial_answer': -1.2586206896551726

Overall mean difference across all answers: -0.817998647734956
Arguments Provided for the Evaluator:
Model:        openbiollm-8B
Label Mode:   gold_label
Output Path:  code/eval_outputs/openbiollm-8B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - openbiollm-8B to csv and json
Model outputs saved to code/eval_outputs/openbiollm-8B/openbiollm-8B_gold_labelled_interpretation_outputs.json and code/eval_outputs/openbiollm-8B/openbiollm-8B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['20448107', '16314619', '17134892', '11261827', '22112969', '19273714']
Number of rows after removing 'Error' or empty string values: 48
Mean difference for 'benefit_answer': 2.833333333333333
Column 'rigor_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'rigor_answer': ['20153039']
Number of rows after removing 'Error' or empty string values: 58
Mean difference for 'rigor_answer': -1.06896551724138
Column 'importance_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'importance_answer': ['17530429', '10637238', '20530276']
Number of rows after removing 'Error' or empty string values: 54
Mean difference for 'importance_answer': -0.7777777777777777
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['10637238', '16314619']
Number of rows after removing 'Error' or empty string values: 56
Mean difference for 'full_text_answer': -0.5714285714285721
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['18794551', '20530276', '20564068', '11261827', '16148021', '18955454', '16314619', '9093724', '16504757', '20673585', '19273714', '17134892', '20153039', '17264336', '15947110', '21399726', '20448107', '20800381', '22112969', '21041710', '20087643']
Number of rows after removing 'Error' or empty string values: 18
Mean difference for 'another_trial_answer': 0.33333333333333304

Overall mean difference across all answers: 0.1496989600437873
Arguments Provided for the Evaluator:
Model:        llama2_chat-7B
Label Mode:   gold_label
Output Path:  code/eval_outputs/llama2_chat-7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-7B to csv and json
Model outputs saved to code/eval_outputs/llama2_chat-7B/llama2_chat-7B_gold_labelled_interpretation_outputs.json and code/eval_outputs/llama2_chat-7B/llama2_chat-7B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 0.833333333333333
Mean difference for 'rigor_answer': -1.0666666666666664
Mean difference for 'importance_answer': 0.033333333333333215
Mean difference for 'full_text_answer': 0.2999999999999998
Mean difference for 'another_trial_answer': -1.3999999999999995

Overall mean difference across all answers: -0.25999999999999995
Arguments Provided for the Evaluator:
Model:        llama2_chat-13B
Label Mode:   gold_label
Output Path:  code/eval_outputs/llama2_chat-13B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 2, 'model.layers.29': 2, 'model.layers.30': 2, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 3, 'model.layers.39': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-13B to csv and json
Model outputs saved to code/eval_outputs/llama2_chat-13B/llama2_chat-13B_gold_labelled_interpretation_outputs.json and code/eval_outputs/llama2_chat-13B/llama2_chat-13B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.0
Mean difference for 'rigor_answer': -1.3666666666666663
Mean difference for 'importance_answer': 0.39999999999999947
Mean difference for 'full_text_answer': 1.5333333333333332
Mean difference for 'another_trial_answer': 1.4

Overall mean difference across all answers: 0.5933333333333333
Arguments Provided for the Evaluator:
Model:        llama3_instruct-8B
Label Mode:   gold_label
Output Path:  code/eval_outputs/llama3_instruct-8B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama3_instruct-8B to csv and json
Model outputs saved to code/eval_outputs/llama3_instruct-8B/llama3_instruct-8B_gold_labelled_interpretation_outputs.json and code/eval_outputs/llama3_instruct-8B/llama3_instruct-8B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.5999999999999996
Mean difference for 'rigor_answer': -1.2000000000000002
Mean difference for 'importance_answer': -0.09999999999999964
Mean difference for 'full_text_answer': 0.06666666666666732
Mean difference for 'another_trial_answer': 0.4666666666666668

Overall mean difference across all answers: 0.1666666666666668
Arguments Provided for the Evaluator:
Model:        llama2_chat-70B
Label Mode:   gold_label
Output Path:  code/eval_outputs/llama2_chat-70B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 1, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 2, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-70B to csv and json
Model outputs saved to code/eval_outputs/llama2_chat-70B/llama2_chat-70B_gold_labelled_interpretation_outputs.json and code/eval_outputs/llama2_chat-70B/llama2_chat-70B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['17179098', '21060024', '20087643', '20564068', '18794551', '17264336', '21471562', '18955454', '11261827', '20153039', '19273714']
Number of rows after removing 'Error' or empty string values: 38
Mean difference for 'benefit_answer': 1.8421052631578942
Mean difference for 'rigor_answer': -1.0666666666666673
Mean difference for 'importance_answer': -0.20000000000000018
Mean difference for 'full_text_answer': 0.06666666666666643
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['20087643', '20564068', '21399726', '18794551', '17264336', '20800381', '10547391', '18955454', '20153039', '19273714']
Number of rows after removing 'Error' or empty string values: 40
Mean difference for 'another_trial_answer': 0.6500000000000004

Overall mean difference across all answers: 0.2584210526315787
Arguments Provided for the Evaluator:
Model:        llama3_instruct-70B
Label Mode:   gold_label
Output Path:  code/eval_outputs/llama3_instruct-70B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama3_instruct-70B to csv and json
Model outputs saved to code/eval_outputs/llama3_instruct-70B/llama3_instruct-70B_gold_labelled_interpretation_outputs.json and code/eval_outputs/llama3_instruct-70B/llama3_instruct-70B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 3.966666666666667
Mean difference for 'rigor_answer': -1.3999999999999995
Mean difference for 'importance_answer': -0.09999999999999964
Mean difference for 'full_text_answer': 2.333333333333333
Mean difference for 'another_trial_answer': 2.566666666666667

Overall mean difference across all answers: 1.4733333333333336
Arguments Provided for the Evaluator:
Model:        med42-70B
Label Mode:   gold_label
Output Path:  code/eval_outputs/med42-70B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - med42-70B to csv and json
Model outputs saved to code/eval_outputs/med42-70B/med42-70B_gold_labelled_interpretation_outputs.json and code/eval_outputs/med42-70B/med42-70B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 3.7
Mean difference for 'rigor_answer': -1.2333333333333334
Mean difference for 'importance_answer': -1.5666666666666673
Mean difference for 'full_text_answer': 0.1333333333333333
Mean difference for 'another_trial_answer': 3.0

Overall mean difference across all answers: 0.8066666666666665
Arguments Provided for the Evaluator:
Model:        openbiollm-70B
Label Mode:   gold_label
Output Path:  code/eval_outputs/openbiollm-70B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - openbiollm-70B to csv and json
Model outputs saved to code/eval_outputs/openbiollm-70B/openbiollm-70B_gold_labelled_interpretation_outputs.json and code/eval_outputs/openbiollm-70B/openbiollm-70B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.933333333333333
Mean difference for 'rigor_answer': -0.833333333333333
Mean difference for 'importance_answer': -0.2333333333333334
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['17173959', '20973267', '16314619', '20530276', '21041710', '10637238', '16504757', '20564068', '16148021', '12177098', '22112969', '18955454', '9093724', '18794551', '17179098', '20087643', '15947110', '21471562', '20153039', '21060024']
Number of rows after removing 'Error' or empty string values: 20
Mean difference for 'full_text_answer': 1.0
Mean difference for 'another_trial_answer': 3.733333333333333

Overall mean difference across all answers: 1.3199999999999998
Arguments Provided for the Evaluator:
Model:        biomedgpt7B
Label Mode:   gold_label
Output Path:  code/eval_outputs/biomedgpt7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - biomedgpt7B to csv and json
Model outputs saved to code/eval_outputs/biomedgpt7B/biomedgpt7B_gold_labelled_interpretation_outputs.json and code/eval_outputs/biomedgpt7B/biomedgpt7B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['18955454', '16314619', '17134892', '20800381', '11261827', '22112969', '15947110', '19273714', '10637238', '20153039', '21471562', '9093724', '10547391', '16504757', '21399726', '21041710', '16148021', '20973267', '20087643', '18794551', '17179098', '12177098', '20530276', '17530429', '21060024', '20448107', '20564068', '17173959', '17264336']
Number of rows after removing 'Error' or empty string values: 2
Mean difference for 'benefit_answer': 0.0
Column 'rigor_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'rigor_answer': ['18955454', '16314619', '17134892', '11261827', '22112969', '15947110', '19273714', '10637238', '20153039', '21471562', '9093724', '20673585', '10547391', '16504757', '21399726', '21041710', '16148021', '20973267', '20087643', '18794551', '17179098', '12177098', '20530276', '17530429', '21060024', '20448107', '20564068', '17173959', '17264336']
Number of rows after removing 'Error' or empty string values: 2
Mean difference for 'rigor_answer': 0.0
Column 'importance_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'importance_answer': ['16314619', '17134892', '20800381', '11261827', '22112969', '15947110', '19273714', '10637238', '20153039', '21471562', '16504757', '21399726', '21041710', '16148021', '20973267', '20087643', '18794551', '17179098', '12177098', '20530276', '21060024', '20448107', '20564068', '17173959', '17264336']
Number of rows after removing 'Error' or empty string values: 10
Mean difference for 'importance_answer': 0.0
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['16314619', '17134892', '20800381', '11261827', '22112969', '15947110', '19273714', '10637238', '20153039', '21471562', '9093724', '20673585', '10547391', '16504757', '21399726', '21041710', '16148021', '20973267', '20087643', '18794551', '17179098', '12177098', '20530276', '21060024', '20448107', '20564068', '17264336']
Number of rows after removing 'Error' or empty string values: 6
Mean difference for 'full_text_answer': -2.333333333333333
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['18955454', '16314619', '17134892', '20800381', '11261827', '22112969', '15947110', '19273714', '10637238', '20153039', '21471562', '9093724', '20673585', '10547391', '16504757', '21399726', '21041710', '16148021', '20973267', '20087643', '18794551', '17179098', '12177098', '20530276', '17530429', '21060024', '20448107', '20564068', '17173959', '17264336']
Number of rows after removing 'Error' or empty string values: 0
Mean difference for 'another_trial_answer': nan

Overall mean difference across all answers: nan
Arguments Provided for the Evaluator:
Model:        alpacare-7B
Label Mode:   gold_label
Output Path:  code/eval_outputs/alpacare-7B
Is Debug:     None

Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - alpacare-7B to csv and json
Model outputs saved to code/eval_outputs/alpacare-7B/alpacare-7B_gold_labelled_interpretation_outputs.json and code/eval_outputs/alpacare-7B/alpacare-7B_gold_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.6
Mean difference for 'rigor_answer': -1.5333333333333332
Mean difference for 'importance_answer': -0.40000000000000036
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['21060024', '21471562', '16148021', '22112969', '20087643', '17530429', '20673585', '17173959']
Number of rows after removing 'Error' or empty string values: 44
Mean difference for 'full_text_answer': 0.0
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['21471562', '16148021', '15947110', '17530429', '10637238', '11261827', '17173959', '12177098', '17264336', '20800381', '17179098', '10547391', '20087643', '20673585', '21060024', '20153039', '20973267', '22112969', '21041710', '20564068', '17134892', '18794551', '19273714', '20530276', '9093724', '21399726', '18955454', '16314619', '16504757']
Number of rows after removing 'Error' or empty string values: 2
Mean difference for 'another_trial_answer': 0.0

Overall mean difference across all answers: -0.0666666666666667
####################################
Running evaluation for interpreting trial results with model output's spin/no spin labels...
Arguments Provided for the Evaluator:
Model:        gpt35
Label Mode:   model_output_label
Output Path:  code/eval_outputs/gpt35
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Saving outputs from model - gpt35 to csv and json
Model outputs saved to code/eval_outputs/gpt35/gpt35_model_output_labelled_interpretation_outputs.json and code/eval_outputs/gpt35/gpt35_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 3.8
Mean difference for 'rigor_answer': 0.36666666666666625
Mean difference for 'importance_answer': 2.033333333333333
Mean difference for 'full_text_answer': 2.033333333333333
Mean difference for 'another_trial_answer': 3.3000000000000003

Overall mean difference across all answers: 2.3066666666666666
Arguments Provided for the Evaluator:
Model:        gpt4o
Label Mode:   model_output_label
Output Path:  code/eval_outputs/gpt4o
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Saving outputs from model - gpt4o to csv and json
Model outputs saved to code/eval_outputs/gpt4o/gpt4o_model_output_labelled_interpretation_outputs.json and code/eval_outputs/gpt4o/gpt4o_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.9333333333333331
Mean difference for 'rigor_answer': -0.13333333333333286
Mean difference for 'importance_answer': 0.8333333333333339
Mean difference for 'full_text_answer': 1.7999999999999998
Mean difference for 'another_trial_answer': 2.5000000000000004

Overall mean difference across all answers: 1.386666666666667
Arguments Provided for the Evaluator:
Model:        gpt4o-mini
Label Mode:   model_output_label
Output Path:  code/eval_outputs/gpt4o-mini
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/eval_outputs/gpt4o-mini/gpt4o-mini_model_output_labelled_interpretation_outputs.json and code/eval_outputs/gpt4o-mini/gpt4o-mini_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.266666666666667
Mean difference for 'rigor_answer': 0.13333333333333286
Mean difference for 'importance_answer': 1.2000000000000002
Mean difference for 'full_text_answer': 2.033333333333333
Mean difference for 'another_trial_answer': 2.066666666666667

Overall mean difference across all answers: 1.54
Arguments Provided for the Evaluator:
Model:        gemini_1.5_flash
Label Mode:   model_output_label
Output Path:  code/eval_outputs/gemini_1.5_flash
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Saving outputs from model - gemini_1.5_flash to csv and json
Model outputs saved to code/eval_outputs/gemini_1.5_flash/gemini_1.5_flash_model_output_labelled_interpretation_outputs.json and code/eval_outputs/gemini_1.5_flash/gemini_1.5_flash_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.2333333333333334
Mean difference for 'rigor_answer': -0.9000000000000004
Mean difference for 'importance_answer': 0.8000000000000003
Mean difference for 'full_text_answer': 0.7666666666666666
Mean difference for 'another_trial_answer': 1.4333333333333336

Overall mean difference across all answers: 0.6666666666666667
Arguments Provided for the Evaluator:
Model:        gemini_1.5_flash-8B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/gemini_1.5_flash-8B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Saving outputs from model - gemini_1.5_flash-8B to csv and json
Model outputs saved to code/eval_outputs/gemini_1.5_flash-8B/gemini_1.5_flash-8B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/gemini_1.5_flash-8B/gemini_1.5_flash-8B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.833333333333333
Mean difference for 'rigor_answer': -1.0
Mean difference for 'importance_answer': 0.33333333333333304
Mean difference for 'full_text_answer': 0.8999999999999999
Mean difference for 'another_trial_answer': 1.3333333333333335

Overall mean difference across all answers: 0.6799999999999999
Arguments Provided for the Evaluator:
Model:        claude_3.5-sonnet
Label Mode:   model_output_label
Output Path:  code/eval_outputs/claude_3.5-sonnet
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Saving outputs from model - claude_3.5-sonnet to csv and json
Model outputs saved to code/eval_outputs/claude_3.5-sonnet/claude_3.5-sonnet_model_output_labelled_interpretation_outputs.json and code/eval_outputs/claude_3.5-sonnet/claude_3.5-sonnet_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.6333333333333337
Mean difference for 'rigor_answer': -0.6333333333333329
Mean difference for 'importance_answer': -1.166666666666666
Mean difference for 'full_text_answer': 3.0
Mean difference for 'another_trial_answer': 1.9333333333333331

Overall mean difference across all answers: 0.9533333333333336
Arguments Provided for the Evaluator:
Model:        claude_3.5-haiku
Label Mode:   model_output_label
Output Path:  code/eval_outputs/claude_3.5-haiku
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Saving outputs from model - claude_3.5-haiku to csv and json
Model outputs saved to code/eval_outputs/claude_3.5-haiku/claude_3.5-haiku_model_output_labelled_interpretation_outputs.json and code/eval_outputs/claude_3.5-haiku/claude_3.5-haiku_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.233333333333333
Mean difference for 'rigor_answer': -0.7000000000000002
Mean difference for 'importance_answer': 0.8999999999999995
Mean difference for 'full_text_answer': 1.166666666666667
Mean difference for 'another_trial_answer': 1.9333333333333327

Overall mean difference across all answers: 1.1066666666666665
Arguments Provided for the Evaluator:
Model:        olmo2_instruct-7B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/olmo2_instruct-7B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - olmo2_instruct-7B to csv and json
Model outputs saved to code/eval_outputs/olmo2_instruct-7B/olmo2_instruct-7B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/olmo2_instruct-7B/olmo2_instruct-7B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['20530276', '18794551', '15947110', '17530429', '20800381']
Number of rows after removing 'Error' or empty string values: 50
Mean difference for 'benefit_answer': 3.3600000000000003
Column 'rigor_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'rigor_answer': ['20530276', '18794551', '20673585', '20800381']
Number of rows after removing 'Error' or empty string values: 52
Mean difference for 'rigor_answer': -0.9615384615384617
Mean difference for 'importance_answer': 1.5
Mean difference for 'full_text_answer': 2.0999999999999996
Mean difference for 'another_trial_answer': 2.3666666666666663

Overall mean difference across all answers: 1.673025641025641
Arguments Provided for the Evaluator:
Model:        olmo2_instruct-13B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/olmo2_instruct-13B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 2, 'model.layers.29': 2, 'model.layers.30': 2, 'model.layers.31': 2, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 3, 'model.layers.39': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - olmo2_instruct-13B to csv and json
Model outputs saved to code/eval_outputs/olmo2_instruct-13B/olmo2_instruct-13B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/olmo2_instruct-13B/olmo2_instruct-13B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 4.633333333333333
Mean difference for 'rigor_answer': -0.2333333333333334
Mean difference for 'importance_answer': 1.3666666666666663
Mean difference for 'full_text_answer': 1.9000000000000004
Mean difference for 'another_trial_answer': 2.7666666666666666

Overall mean difference across all answers: 2.0866666666666664
Arguments Provided for the Evaluator:
Model:        mistral_instruct7B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/mistral_instruct7B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - mistral_instruct7B to csv and json
Model outputs saved to code/eval_outputs/mistral_instruct7B/mistral_instruct7B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/mistral_instruct7B/mistral_instruct7B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.8666666666666663
Mean difference for 'rigor_answer': -0.13333333333333286
Mean difference for 'importance_answer': 1.5333333333333332
Mean difference for 'full_text_answer': 2.6000000000000005
Mean difference for 'another_trial_answer': 2.333333333333333

Overall mean difference across all answers: 1.84
Arguments Provided for the Evaluator:
Model:        med42-8B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/med42-8B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - med42-8B to csv and json
Model outputs saved to code/eval_outputs/med42-8B/med42-8B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/med42-8B/med42-8B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.1333333333333333
Mean difference for 'rigor_answer': 0.0
Mean difference for 'importance_answer': 0.9000000000000004
Mean difference for 'full_text_answer': 1.0
Mean difference for 'another_trial_answer': 2.9

Overall mean difference across all answers: 1.3866666666666667
Arguments Provided for the Evaluator:
Model:        biomistral7B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/biomistral7B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - biomistral7B to csv and json
Model outputs saved to code/eval_outputs/biomistral7B/biomistral7B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/biomistral7B/biomistral7B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.7166666666666668
Mean difference for 'rigor_answer': 0.016666666666667496
Column 'importance_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'importance_answer': ['10637238', '11261827', '16148021', '10547391', '16504757', '20564068', '21060024', '12177098', '17134892', '17179098', '19273714', '18794551', '20800381', '21399726', '18955454', '9093724', '21041710']
Number of rows after removing 'Error' or empty string values: 26
Mean difference for 'importance_answer': 0.6538461538461533
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['16504757', '16148021', '10547391', '20564068', '12177098', '17179098', '19273714', '20087643', '16314619', '20530276', '18794551', '20800381', '20973267', '18955454', '9093724', '22112969']
Number of rows after removing 'Error' or empty string values: 28
Mean difference for 'full_text_answer': 0.6071428571428559
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['20973267', '10547391']
Number of rows after removing 'Error' or empty string values: 56
Mean difference for 'another_trial_answer': 0.6607142857142856

Overall mean difference across all answers: 0.7310073260073258
Arguments Provided for the Evaluator:
Model:        openbiollm-8B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/openbiollm-8B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - openbiollm-8B to csv and json
Model outputs saved to code/eval_outputs/openbiollm-8B/openbiollm-8B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/openbiollm-8B/openbiollm-8B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['21471562', '16314619', '17134892', '11261827', '20448107', '19273714', '22112969']
Number of rows after removing 'Error' or empty string values: 46
Mean difference for 'benefit_answer': 3.304347826086956
Column 'rigor_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'rigor_answer': ['20153039']
Number of rows after removing 'Error' or empty string values: 58
Mean difference for 'rigor_answer': -0.06896551724137989
Column 'importance_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'importance_answer': ['20673585', '17530429', '20530276']
Number of rows after removing 'Error' or empty string values: 54
Mean difference for 'importance_answer': 1.5185185185185182
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['10637238', '16314619', '21471562']
Number of rows after removing 'Error' or empty string values: 54
Mean difference for 'full_text_answer': 2.518518518518519
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['16504757', '15947110', '20800381', '21471562', '20153039', '21060024', '11261827', '17264336', '19273714', '22112969', '18955454', '20530276', '20087643', '21399726', '20448107', '18794551', '20673585', '16314619', '17134892', '21041710']
Number of rows after removing 'Error' or empty string values: 20
Mean difference for 'another_trial_answer': 1.7999999999999998

Overall mean difference across all answers: 1.8144838691765226
Arguments Provided for the Evaluator:
Model:        llama2_chat-7B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/llama2_chat-7B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-7B to csv and json
Model outputs saved to code/eval_outputs/llama2_chat-7B/llama2_chat-7B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/llama2_chat-7B/llama2_chat-7B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 2.3
Mean difference for 'rigor_answer': 0.16666666666666696
Mean difference for 'importance_answer': 0.2999999999999998
Mean difference for 'full_text_answer': 0.6333333333333337
Mean difference for 'another_trial_answer': 1.3000000000000003

Overall mean difference across all answers: 0.9400000000000002
Arguments Provided for the Evaluator:
Model:        llama2_chat-13B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/llama2_chat-13B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 2, 'model.layers.29': 2, 'model.layers.30': 2, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 3, 'model.layers.39': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-13B to csv and json
Model outputs saved to code/eval_outputs/llama2_chat-13B/llama2_chat-13B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/llama2_chat-13B/llama2_chat-13B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.1
Mean difference for 'rigor_answer': 0.033333333333333215
Mean difference for 'importance_answer': 0.49999999999999956
Mean difference for 'full_text_answer': 1.6666666666666665
Mean difference for 'another_trial_answer': 1.8666666666666667

Overall mean difference across all answers: 1.0333333333333332
Arguments Provided for the Evaluator:
Model:        llama3_instruct-8B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/llama3_instruct-8B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama3_instruct-8B to csv and json
Model outputs saved to code/eval_outputs/llama3_instruct-8B/llama3_instruct-8B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/llama3_instruct-8B/llama3_instruct-8B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 1.833333333333333
Mean difference for 'rigor_answer': -0.13333333333333286
Mean difference for 'importance_answer': 0.5666666666666664
Mean difference for 'full_text_answer': 0.7000000000000002
Mean difference for 'another_trial_answer': 1.4333333333333336

Overall mean difference across all answers: 0.8800000000000001
Arguments Provided for the Evaluator:
Model:        llama2_chat-70B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/llama2_chat-70B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 1, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 2, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-70B to csv and json
Model outputs saved to code/eval_outputs/llama2_chat-70B/llama2_chat-70B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/llama2_chat-70B/llama2_chat-70B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['18794551', '20153039']
Number of rows after removing 'Error' or empty string values: 56
Mean difference for 'benefit_answer': 2.0
Mean difference for 'rigor_answer': -0.16666666666666607
Mean difference for 'importance_answer': 0.10000000000000053
Mean difference for 'full_text_answer': 0.033333333333333215
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['20153039', '17264336', '10547391', '21399726', '20087643']
Number of rows after removing 'Error' or empty string values: 50
Mean difference for 'another_trial_answer': 0.3600000000000003

Overall mean difference across all answers: 0.4653333333333336
Arguments Provided for the Evaluator:
Model:        llama3_instruct-70B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/llama3_instruct-70B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama3_instruct-70B to csv and json
Model outputs saved to code/eval_outputs/llama3_instruct-70B/llama3_instruct-70B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/llama3_instruct-70B/llama3_instruct-70B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 4.1
Mean difference for 'rigor_answer': -0.2333333333333334
Mean difference for 'importance_answer': 0.8333333333333339
Mean difference for 'full_text_answer': 3.033333333333333
Mean difference for 'another_trial_answer': 3.733333333333333

Overall mean difference across all answers: 2.2933333333333334
Arguments Provided for the Evaluator:
Model:        med42-70B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/med42-70B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - med42-70B to csv and json
Model outputs saved to code/eval_outputs/med42-70B/med42-70B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/med42-70B/med42-70B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 4.5
Mean difference for 'rigor_answer': 0.09999999999999964
Mean difference for 'importance_answer': 1.0666666666666664
Mean difference for 'full_text_answer': 3.1
Mean difference for 'another_trial_answer': 4.333333333333333

Overall mean difference across all answers: 2.6199999999999997
Arguments Provided for the Evaluator:
Model:        openbiollm-70B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/openbiollm-70B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - openbiollm-70B to csv and json
Model outputs saved to code/eval_outputs/openbiollm-70B/openbiollm-70B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/openbiollm-70B/openbiollm-70B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 3.833333333333333
Mean difference for 'rigor_answer': -0.18333333333333268
Mean difference for 'importance_answer': 1.0
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['21041710', '18794551', '20564068', '19273714', '21471562', '9093724', '16314619', '18955454', '11261827', '10637238', '16504757', '20087643', '20973267', '21060024', '17173959', '17264336', '20153039']
Number of rows after removing 'Error' or empty string values: 26
Mean difference for 'full_text_answer': 0.6153846153846159
Mean difference for 'another_trial_answer': 4.7333333333333325

Overall mean difference across all answers: 1.9997435897435898
Arguments Provided for the Evaluator:
Model:        biomedgpt7B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/biomedgpt7B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - biomedgpt7B to csv and json
Model outputs saved to code/eval_outputs/biomedgpt7B/biomedgpt7B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/biomedgpt7B/biomedgpt7B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Column 'benefit_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'benefit_answer': ['10547391', '17530429', '9093724', '21399726', '17179098', '22112969', '17173959', '20153039', '21041710', '21471562', '18794551', '20973267', '20800381', '12177098', '16148021', '20530276', '10637238', '20087643', '18955454', '15947110', '20564068', '16314619', '21060024', '19273714', '17264336', '17134892', '11261827', '16504757']
Number of rows after removing 'Error' or empty string values: 4
Mean difference for 'benefit_answer': 0.0
Column 'rigor_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'rigor_answer': ['10547391', '20673585', '17530429', '9093724', '21399726', '17179098', '22112969', '17173959', '20153039', '21041710', '21471562', '18794551', '20973267', '12177098', '16148021', '20530276', '10637238', '20087643', '18955454', '15947110', '20564068', '16314619', '21060024', '19273714', '17264336', '17134892', '11261827', '16504757', '20448107']
Number of rows after removing 'Error' or empty string values: 2
Mean difference for 'rigor_answer': -1.0
Column 'importance_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'importance_answer': ['21399726', '17179098', '22112969', '17173959', '20153039', '21041710', '21471562', '18794551', '20973267', '20800381', '12177098', '16148021', '20530276', '10637238', '20087643', '15947110', '20564068', '16314619', '21060024', '19273714', '17264336', '17134892', '11261827', '16504757', '20448107']
Number of rows after removing 'Error' or empty string values: 10
Mean difference for 'importance_answer': 0.0
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['10547391', '20673585', '9093724', '21399726', '17179098', '22112969', '20153039', '21041710', '21471562', '18794551', '20973267', '20800381', '12177098', '16148021', '20530276', '10637238', '20087643', '15947110', '20564068', '16314619', '21060024', '19273714', '17264336', '17134892', '11261827', '16504757', '20448107']
Number of rows after removing 'Error' or empty string values: 6
Mean difference for 'full_text_answer': -2.333333333333333
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['10547391', '20673585', '17530429', '9093724', '21399726', '17179098', '22112969', '17173959', '20153039', '21041710', '21471562', '18794551', '20973267', '20800381', '12177098', '16148021', '20530276', '10637238', '20087643', '18955454', '15947110', '20564068', '16314619', '21060024', '19273714', '17264336', '17134892', '11261827', '16504757', '20448107']
Number of rows after removing 'Error' or empty string values: 0
Mean difference for 'another_trial_answer': nan

Overall mean difference across all answers: nan
Arguments Provided for the Evaluator:
Model:        alpacare-7B
Label Mode:   model_output_label
Output Path:  code/eval_outputs/alpacare-7B
Is Debug:     None

Loading the dataset...
Loading the dataset with spin labels...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - alpacare-7B to csv and json
Model outputs saved to code/eval_outputs/alpacare-7B/alpacare-7B_model_output_labelled_interpretation_outputs.json and code/eval_outputs/alpacare-7B/alpacare-7B_model_output_labelled_interpretation_outputs.csv
Calculating means differences in scores between spin and no spin abstracts...
Mean difference for 'benefit_answer': 4.333333333333333
Mean difference for 'rigor_answer': 0.13333333333333286
Mean difference for 'importance_answer': 0.20000000000000107
Column 'full_text_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'full_text_answer': ['20530276', '21471562', '17173959', '20087643', '22112969', '17530429', '16148021', '20564068', '20673585']
Number of rows after removing 'Error' or empty string values: 42
Mean difference for 'full_text_answer': 0.0
Column 'another_trial_answer' has some 'Error' or empty string values. Removing these rows from the metrics...
PMIDs with 'Error' or empty string values in column 'another_trial_answer': ['15947110', '21060024', '17264336', '20800381', '20564068', '20673585', '20530276', '21399726', '21041710', '16148021', '17179098', '19273714', '11261827', '10637238', '16314619', '18955454', '9093724', '20153039', '17134892', '17173959', '20087643', '22112969', '16504757', '18794551', '20973267', '17530429', '10547391', '12177098', '21471562']
Number of rows after removing 'Error' or empty string values: 2
Mean difference for 'another_trial_answer': 0.0

Overall mean difference across all answers: 0.9333333333333333
