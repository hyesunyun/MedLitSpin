
     active environment : MedLitSpin
    active env location : /home/yun.hy/.conda/envs/MedLitSpin
            shell level : 2
       user config file : /home/yun.hy/.condarc
 populated config files : 
          conda version : 24.5.0
    conda-build version : 24.5.1
         python version : 3.12.4.final.0
                 solver : libmamba (default)
       virtual packages : __archspec=1=sapphirerapids
                          __conda=24.5.0=0
                          __cuda=12.3=0
                          __glibc=2.34=0
                          __linux=5.14.0=0
                          __unix=0=0
       base environment : /shared/EL9/explorer/anaconda3/2024.06  (read only)
      conda av data dir : /shared/EL9/explorer/anaconda3/2024.06/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /shared/EL9/explorer/anaconda3/2024.06/pkgs
                          /home/yun.hy/.conda/pkgs
       envs directories : /home/yun.hy/.conda/envs
                          /shared/EL9/explorer/anaconda3/2024.06/envs
               platform : linux-64
             user-agent : conda/24.5.0 requests/2.32.2 CPython/3.12.4 Linux/5.14.0-362.13.1.el9_3.x86_64 rocky/9.3 glibc/2.34 solver/libmamba conda-libmamba-solver/24.1.0 libmambapy/1.5.8 aau/0.4.4 c/. s/. e/.
                UID:GID : 1825635949:100
             netrc file : /home/yun.hy/.netrc
           offline mode : False


Generating plain language summaries...
Arguments Provided for the Generator:
Model:        gpt35
Output Path:  code/pls_outputs/gpt35
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - gpt35 to csv and json
Model outputs saved to code/pls_outputs/gpt35/gpt35_outputs.json and code/pls_outputs/gpt35/gpt35_outputs.csv
Arguments Provided for the Generator:
Model:        gpt4o
Output Path:  code/pls_outputs/gpt4o
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o to csv and json
Model outputs saved to code/pls_outputs/gpt4o/gpt4o_outputs.json and code/pls_outputs/gpt4o/gpt4o_outputs.csv
Arguments Provided for the Generator:
Model:        gpt4o-mini
Output Path:  code/pls_outputs/gpt4o-mini
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/gpt4o-mini/gpt4o-mini_outputs.json and code/pls_outputs/gpt4o-mini/gpt4o-mini_outputs.csv
Arguments Provided for the Generator:
Model:        gemini_1.5_flash
Output Path:  code/pls_outputs/gemini_1.5_flash
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - gemini_1.5_flash to csv and json
Model outputs saved to code/pls_outputs/gemini_1.5_flash/gemini_1.5_flash_outputs.json and code/pls_outputs/gemini_1.5_flash/gemini_1.5_flash_outputs.csv
Arguments Provided for the Generator:
Model:        gemini_1.5_flash-8B
Output Path:  code/pls_outputs/gemini_1.5_flash-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - gemini_1.5_flash-8B to csv and json
Model outputs saved to code/pls_outputs/gemini_1.5_flash-8B/gemini_1.5_flash-8B_outputs.json and code/pls_outputs/gemini_1.5_flash-8B/gemini_1.5_flash-8B_outputs.csv
Arguments Provided for the Generator:
Model:        claude_3.5-sonnet
Output Path:  code/pls_outputs/claude_3.5-sonnet
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - claude_3.5-sonnet to csv and json
Model outputs saved to code/pls_outputs/claude_3.5-sonnet/claude_3.5-sonnet_outputs.json and code/pls_outputs/claude_3.5-sonnet/claude_3.5-sonnet_outputs.csv
Arguments Provided for the Generator:
Model:        claude_3.5-haiku
Output Path:  code/pls_outputs/claude_3.5-haiku
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - claude_3.5-haiku to csv and json
Model outputs saved to code/pls_outputs/claude_3.5-haiku/claude_3.5-haiku_outputs.json and code/pls_outputs/claude_3.5-haiku/claude_3.5-haiku_outputs.csv
Arguments Provided for the Generator:
Model:        olmo2_instruct-7B
Output Path:  code/pls_outputs/olmo2_instruct-7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - olmo2_instruct-7B to csv and json
Model outputs saved to code/pls_outputs/olmo2_instruct-7B/olmo2_instruct-7B_outputs.json and code/pls_outputs/olmo2_instruct-7B/olmo2_instruct-7B_outputs.csv
Arguments Provided for the Generator:
Model:        olmo2_instruct-13B
Output Path:  code/pls_outputs/olmo2_instruct-13B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 2, 'model.layers.29': 2, 'model.layers.30': 2, 'model.layers.31': 2, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 3, 'model.layers.39': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - olmo2_instruct-13B to csv and json
Model outputs saved to code/pls_outputs/olmo2_instruct-13B/olmo2_instruct-13B_outputs.json and code/pls_outputs/olmo2_instruct-13B/olmo2_instruct-13B_outputs.csv
Arguments Provided for the Generator:
Model:        mistral_instruct7B
Output Path:  code/pls_outputs/mistral_instruct7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - mistral_instruct7B to csv and json
Model outputs saved to code/pls_outputs/mistral_instruct7B/mistral_instruct7B_outputs.json and code/pls_outputs/mistral_instruct7B/mistral_instruct7B_outputs.csv
Arguments Provided for the Generator:
Model:        med42-8B
Output Path:  code/pls_outputs/med42-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - med42-8B to csv and json
Model outputs saved to code/pls_outputs/med42-8B/med42-8B_outputs.json and code/pls_outputs/med42-8B/med42-8B_outputs.csv
Arguments Provided for the Generator:
Model:        med42-70B
Output Path:  code/pls_outputs/med42-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - med42-70B to csv and json
Model outputs saved to code/pls_outputs/med42-70B/med42-70B_outputs.json and code/pls_outputs/med42-70B/med42-70B_outputs.csv
Arguments Provided for the Generator:
Model:        biomistral7B
Output Path:  code/pls_outputs/biomistral7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - biomistral7B to csv and json
Model outputs saved to code/pls_outputs/biomistral7B/biomistral7B_outputs.json and code/pls_outputs/biomistral7B/biomistral7B_outputs.csv
Arguments Provided for the Generator:
Model:        openbiollm-8B
Output Path:  code/pls_outputs/openbiollm-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - openbiollm-8B to csv and json
Model outputs saved to code/pls_outputs/openbiollm-8B/openbiollm-8B_outputs.json and code/pls_outputs/openbiollm-8B/openbiollm-8B_outputs.csv
Arguments Provided for the Generator:
Model:        openbiollm-70B
Output Path:  code/pls_outputs/openbiollm-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - openbiollm-70B to csv and json
Model outputs saved to code/pls_outputs/openbiollm-70B/openbiollm-70B_outputs.json and code/pls_outputs/openbiollm-70B/openbiollm-70B_outputs.csv
Arguments Provided for the Generator:
Model:        llama2_chat-7B
Output Path:  code/pls_outputs/llama2_chat-7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-7B to csv and json
Model outputs saved to code/pls_outputs/llama2_chat-7B/llama2_chat-7B_outputs.json and code/pls_outputs/llama2_chat-7B/llama2_chat-7B_outputs.csv
Arguments Provided for the Generator:
Model:        llama2_chat-13B
Output Path:  code/pls_outputs/llama2_chat-13B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 2, 'model.layers.29': 2, 'model.layers.30': 2, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 3, 'model.layers.39': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-13B to csv and json
Model outputs saved to code/pls_outputs/llama2_chat-13B/llama2_chat-13B_outputs.json and code/pls_outputs/llama2_chat-13B/llama2_chat-13B_outputs.csv
Arguments Provided for the Generator:
Model:        llama2_chat-70B
Output Path:  code/pls_outputs/llama2_chat-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 1, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 2, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-70B to csv and json
Model outputs saved to code/pls_outputs/llama2_chat-70B/llama2_chat-70B_outputs.json and code/pls_outputs/llama2_chat-70B/llama2_chat-70B_outputs.csv
Arguments Provided for the Generator:
Model:        llama3_instruct-8B
Output Path:  code/pls_outputs/llama3_instruct-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama3_instruct-8B to csv and json
Model outputs saved to code/pls_outputs/llama3_instruct-8B/llama3_instruct-8B_outputs.json and code/pls_outputs/llama3_instruct-8B/llama3_instruct-8B_outputs.csv
Arguments Provided for the Generator:
Model:        llama3_instruct-70B
Output Path:  code/pls_outputs/llama3_instruct-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama3_instruct-70B to csv and json
Model outputs saved to code/pls_outputs/llama3_instruct-70B/llama3_instruct-70B_outputs.json and code/pls_outputs/llama3_instruct-70B/llama3_instruct-70B_outputs.csv
Arguments Provided for the Generator:
Model:        biomedgpt7B
Output Path:  code/pls_outputs/biomedgpt7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - biomedgpt7B to csv and json
Model outputs saved to code/pls_outputs/biomedgpt7B/biomedgpt7B_outputs.json and code/pls_outputs/biomedgpt7B/biomedgpt7B_outputs.csv
Arguments Provided for the Generator:
Model:        alpacare-7B
Output Path:  code/pls_outputs/alpacare-7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Output path did not exist. Directory was created.
Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - alpacare-7B to csv and json
Model outputs saved to code/pls_outputs/alpacare-7B/alpacare-7B_outputs.json and code/pls_outputs/alpacare-7B/alpacare-7B_outputs.csv
####################################
Generating plain language summaries with ground truth spin/no spin labels...
Arguments Provided for the Generator:
Model:        gpt35
Label Mode:   gold_label
Output Path:  code/pls_outputs/gpt35
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - gpt35 to csv and json
Model outputs saved to code/pls_outputs/gpt35/gpt35_gold_labelled_outputs.json and code/pls_outputs/gpt35/gpt35_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        gpt4o
Label Mode:   gold_label
Output Path:  code/pls_outputs/gpt4o
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o to csv and json
Model outputs saved to code/pls_outputs/gpt4o/gpt4o_gold_labelled_outputs.json and code/pls_outputs/gpt4o/gpt4o_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        gpt4o-mini
Label Mode:   gold_label
Output Path:  code/pls_outputs/gpt4o-mini
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - gpt4o-mini to csv and json
Model outputs saved to code/pls_outputs/gpt4o-mini/gpt4o-mini_gold_labelled_outputs.json and code/pls_outputs/gpt4o-mini/gpt4o-mini_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        gemini_1.5_flash
Label Mode:   gold_label
Output Path:  code/pls_outputs/gemini_1.5_flash
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - gemini_1.5_flash to csv and json
Model outputs saved to code/pls_outputs/gemini_1.5_flash/gemini_1.5_flash_gold_labelled_outputs.json and code/pls_outputs/gemini_1.5_flash/gemini_1.5_flash_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        gemini_1.5_flash-8B
Label Mode:   gold_label
Output Path:  code/pls_outputs/gemini_1.5_flash-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - gemini_1.5_flash-8B to csv and json
Model outputs saved to code/pls_outputs/gemini_1.5_flash-8B/gemini_1.5_flash-8B_gold_labelled_outputs.json and code/pls_outputs/gemini_1.5_flash-8B/gemini_1.5_flash-8B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        claude_3.5-sonnet
Label Mode:   gold_label
Output Path:  code/pls_outputs/claude_3.5-sonnet
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - claude_3.5-sonnet to csv and json
Model outputs saved to code/pls_outputs/claude_3.5-sonnet/claude_3.5-sonnet_gold_labelled_outputs.json and code/pls_outputs/claude_3.5-sonnet/claude_3.5-sonnet_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        claude_3.5-haiku
Label Mode:   gold_label
Output Path:  code/pls_outputs/claude_3.5-haiku
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Saving outputs from model - claude_3.5-haiku to csv and json
Model outputs saved to code/pls_outputs/claude_3.5-haiku/claude_3.5-haiku_gold_labelled_outputs.json and code/pls_outputs/claude_3.5-haiku/claude_3.5-haiku_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        olmo2_instruct-7B
Label Mode:   gold_label
Output Path:  code/pls_outputs/olmo2_instruct-7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - olmo2_instruct-7B to csv and json
Model outputs saved to code/pls_outputs/olmo2_instruct-7B/olmo2_instruct-7B_gold_labelled_outputs.json and code/pls_outputs/olmo2_instruct-7B/olmo2_instruct-7B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        olmo2_instruct-13B
Label Mode:   gold_label
Output Path:  code/pls_outputs/olmo2_instruct-13B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 2, 'model.layers.29': 2, 'model.layers.30': 2, 'model.layers.31': 2, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 3, 'model.layers.39': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - olmo2_instruct-13B to csv and json
Model outputs saved to code/pls_outputs/olmo2_instruct-13B/olmo2_instruct-13B_gold_labelled_outputs.json and code/pls_outputs/olmo2_instruct-13B/olmo2_instruct-13B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        mistral_instruct7B
Label Mode:   gold_label
Output Path:  code/pls_outputs/mistral_instruct7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - mistral_instruct7B to csv and json
Model outputs saved to code/pls_outputs/mistral_instruct7B/mistral_instruct7B_gold_labelled_outputs.json and code/pls_outputs/mistral_instruct7B/mistral_instruct7B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        med42-8B
Label Mode:   gold_label
Output Path:  code/pls_outputs/med42-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - med42-8B to csv and json
Model outputs saved to code/pls_outputs/med42-8B/med42-8B_gold_labelled_outputs.json and code/pls_outputs/med42-8B/med42-8B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        med42-70B
Label Mode:   gold_label
Output Path:  code/pls_outputs/med42-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - med42-70B to csv and json
Model outputs saved to code/pls_outputs/med42-70B/med42-70B_gold_labelled_outputs.json and code/pls_outputs/med42-70B/med42-70B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        biomistral7B
Label Mode:   gold_label
Output Path:  code/pls_outputs/biomistral7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}

Saving outputs from model - biomistral7B to csv and json
Model outputs saved to code/pls_outputs/biomistral7B/biomistral7B_gold_labelled_outputs.json and code/pls_outputs/biomistral7B/biomistral7B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        openbiollm-8B
Label Mode:   gold_label
Output Path:  code/pls_outputs/openbiollm-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - openbiollm-8B to csv and json
Model outputs saved to code/pls_outputs/openbiollm-8B/openbiollm-8B_gold_labelled_outputs.json and code/pls_outputs/openbiollm-8B/openbiollm-8B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        openbiollm-70B
Label Mode:   gold_label
Output Path:  code/pls_outputs/openbiollm-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - openbiollm-70B to csv and json
Model outputs saved to code/pls_outputs/openbiollm-70B/openbiollm-70B_gold_labelled_outputs.json and code/pls_outputs/openbiollm-70B/openbiollm-70B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        llama2_chat-7B
Label Mode:   gold_label
Output Path:  code/pls_outputs/llama2_chat-7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-7B to csv and json
Model outputs saved to code/pls_outputs/llama2_chat-7B/llama2_chat-7B_gold_labelled_outputs.json and code/pls_outputs/llama2_chat-7B/llama2_chat-7B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        llama2_chat-13B
Label Mode:   gold_label
Output Path:  code/pls_outputs/llama2_chat-13B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 2, 'model.layers.26': 2, 'model.layers.27': 2, 'model.layers.28': 2, 'model.layers.29': 2, 'model.layers.30': 2, 'model.layers.31': 3, 'model.layers.32': 3, 'model.layers.33': 3, 'model.layers.34': 3, 'model.layers.35': 3, 'model.layers.36': 3, 'model.layers.37': 3, 'model.layers.38': 3, 'model.layers.39': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-13B to csv and json
Model outputs saved to code/pls_outputs/llama2_chat-13B/llama2_chat-13B_gold_labelled_outputs.json and code/pls_outputs/llama2_chat-13B/llama2_chat-13B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        llama2_chat-70B
Label Mode:   gold_label
Output Path:  code/pls_outputs/llama2_chat-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 1, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 2, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama2_chat-70B to csv and json
Model outputs saved to code/pls_outputs/llama2_chat-70B/llama2_chat-70B_gold_labelled_outputs.json and code/pls_outputs/llama2_chat-70B/llama2_chat-70B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        llama3_instruct-8B
Label Mode:   gold_label
Output Path:  code/pls_outputs/llama3_instruct-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama3_instruct-8B to csv and json
Model outputs saved to code/pls_outputs/llama3_instruct-8B/llama3_instruct-8B_gold_labelled_outputs.json and code/pls_outputs/llama3_instruct-8B/llama3_instruct-8B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        llama3_instruct-70B
Label Mode:   gold_label
Output Path:  code/pls_outputs/llama3_instruct-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.bfloat16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 2, 'model.layers.41': 2, 'model.layers.42': 2, 'model.layers.43': 2, 'model.layers.44': 2, 'model.layers.45': 2, 'model.layers.46': 2, 'model.layers.47': 2, 'model.layers.48': 2, 'model.layers.49': 2, 'model.layers.50': 2, 'model.layers.51': 2, 'model.layers.52': 2, 'model.layers.53': 2, 'model.layers.54': 2, 'model.layers.55': 2, 'model.layers.56': 2, 'model.layers.57': 2, 'model.layers.58': 2, 'model.layers.59': 2, 'model.layers.60': 2, 'model.layers.61': 3, 'model.layers.62': 3, 'model.layers.63': 3, 'model.layers.64': 3, 'model.layers.65': 3, 'model.layers.66': 3, 'model.layers.67': 3, 'model.layers.68': 3, 'model.layers.69': 3, 'model.layers.70': 3, 'model.layers.71': 3, 'model.layers.72': 3, 'model.layers.73': 3, 'model.layers.74': 3, 'model.layers.75': 3, 'model.layers.76': 3, 'model.layers.77': 3, 'model.layers.78': 3, 'model.layers.79': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - llama3_instruct-70B to csv and json
Model outputs saved to code/pls_outputs/llama3_instruct-70B/llama3_instruct-70B_gold_labelled_outputs.json and code/pls_outputs/llama3_instruct-70B/llama3_instruct-70B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        biomedgpt7B
Label Mode:   gold_label
Output Path:  code/pls_outputs/biomedgpt7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - biomedgpt7B to csv and json
Model outputs saved to code/pls_outputs/biomedgpt7B/biomedgpt7B_gold_labelled_outputs.json and code/pls_outputs/biomedgpt7B/biomedgpt7B_gold_labelled_outputs.csv
Arguments Provided for the Generator:
Model:        alpacare-7B
Label Mode:   gold_label
Output Path:  code/pls_outputs/alpacare-7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the model...
Model's dtype: torch.float16
Model's device: cuda:0
Model's device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

Saving outputs from model - alpacare-7B to csv and json
Model outputs saved to code/pls_outputs/alpacare-7B/alpacare-7B_gold_labelled_outputs.json and code/pls_outputs/alpacare-7B/alpacare-7B_gold_labelled_outputs.csv
####################################
Generating plain language summaries with model output's spin/no spin labels...
Arguments Provided for the Generator:
Model:        gpt35
Label Mode:   model_output_label
Output Path:  code/pls_outputs/gpt35
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        gpt4o
Label Mode:   model_output_label
Output Path:  code/pls_outputs/gpt4o
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        gpt4o-mini
Label Mode:   model_output_label
Output Path:  code/pls_outputs/gpt4o-mini
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        gemini_1.5_flash
Label Mode:   model_output_label
Output Path:  code/pls_outputs/gemini_1.5_flash
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        gemini_1.5_flash-8B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/gemini_1.5_flash-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        claude_3.5-sonnet
Label Mode:   model_output_label
Output Path:  code/pls_outputs/claude_3.5-sonnet
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        claude_3.5-haiku
Label Mode:   model_output_label
Output Path:  code/pls_outputs/claude_3.5-haiku
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        olmo2_instruct-7B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/olmo2_instruct-7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        olmo2_instruct-13B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/olmo2_instruct-13B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        mistral_instruct7B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/mistral_instruct7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        med42-8B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/med42-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        med42-70B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/med42-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        biomistral7B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/biomistral7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        openbiollm-8B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/openbiollm-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        openbiollm-70B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/openbiollm-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        llama2_chat-7B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/llama2_chat-7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        llama2_chat-13B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/llama2_chat-13B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        llama2_chat-70B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/llama2_chat-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        llama3_instruct-8B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/llama3_instruct-8B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        llama3_instruct-70B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/llama3_instruct-70B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        biomedgpt7B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/biomedgpt7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
Arguments Provided for the Generator:
Model:        alpacare-7B
Label Mode:   model_output_label
Output Path:  code/pls_outputs/alpacare-7B
Max Output Tokens:   300
Prompt Template:     default
Is Debug:     None

Loading the prompt template...
Loading the dataset...
Loading the dataset with spin labels...
